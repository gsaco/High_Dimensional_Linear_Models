{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Part 1: Frisch-Waugh-Lovell (FWL) Theorem\n",
    "## Math (3 points)\n",
    "\n",
    "This notebook contains the mathematical proof and numerical verification of the Frisch-Waugh-Lovell theorem implemented in Julia.\n",
    "\n",
    "The FWL theorem is a fundamental result in econometrics that shows how to isolate the effect of specific variables by \"partialling out\" the effects of other variables.\n",
    "\n",
    "Julia provides excellent performance for numerical linear algebra operations, making it ideal for implementing econometric methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Random\n",
    "using Printf\n",
    "using Statistics\n",
    "using Plots\n",
    "\n",
    "# Set default plot backend\n",
    "gr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Proof of the FWL Theorem\n",
    "\n",
    "The FWL theorem states that the OLS estimate of β₁ in the regression of y on [X₁ X₂] is equal to the OLS estimate obtained from the following two-step procedure:\n",
    "\n",
    "1. Regress y on X₂ and obtain the residuals ỹ = M_{X₂}y, where M_{X₂} = I - X₂(X₂'X₂)⁻¹X₂'\n",
    "2. Regress X₁ on X₂ and obtain the residuals X̃₁ = M_{X₂}X₁\n",
    "3. Regress ỹ on X̃₁ and show that the resulting coefficient vector is equal to β̂₁ from the full regression.\n",
    "\n",
    "Formally, we need to show that: β̂₁ = (X̃₁'X̃₁)⁻¹X̃₁'ỹ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fwl_theorem_proof()\n",
    "    \"\"\"\n",
    "    Mathematical proof of the Frisch-Waugh-Lovell theorem.\n",
    "    \"\"\"\n",
    "    println(\"=== FRISCH-WAUGH-LOVELL THEOREM PROOF ===\\n\")\n",
    "    \n",
    "    println(\"Mathematical Proof:\")\n",
    "    println(\"==================\")\n",
    "    println()\n",
    "    println(\"Consider the linear regression model:\")\n",
    "    println(\"y = X₁β₁ + X₂β₂ + u\")\n",
    "    println()\n",
    "    println(\"Where:\")\n",
    "    println(\"- y is an n×1 vector of outcomes\")\n",
    "    println(\"- X₁ is an n×k₁ matrix of regressors of interest\")\n",
    "    println(\"- X₂ is an n×k₂ matrix of control variables\")\n",
    "    println(\"- u is an n×1 vector of errors\")\n",
    "    println()\n",
    "    \n",
    "    println(\"Step 1: Full regression\")\n",
    "    println(\"The full regression in matrix form is:\")\n",
    "    println(\"y = [X₁ X₂][β₁; β₂] + u = Xβ + u\")\n",
    "    println()\n",
    "    println(\"The OLS estimator is:\")\n",
    "    println(\"β̂ = (X'X)⁻¹X'y\")\n",
    "    println()\n",
    "    println(\"Partitioning X'X and X'y:\")\n",
    "    println(\"X'X = [X₁'X₁  X₁'X₂]\")\n",
    "    println(\"      [X₂'X₁  X₂'X₂]\")\n",
    "    println()\n",
    "    println(\"X'y = [X₁'y]\")\n",
    "    println(\"      [X₂'y]\")\n",
    "    println()\n",
    "    \n",
    "    println(\"Step 2: Using the partitioned inverse formula\")\n",
    "    println(\"For a partitioned matrix [A B; C D], if D is invertible:\")\n",
    "    println(\"The (1,1) block of the inverse is (A - BD⁻¹C)⁻¹\")\n",
    "    println()\n",
    "    println(\"Applying this to our case:\")\n",
    "    println(\"β̂₁ = [(X₁'X₁ - X₁'X₂(X₂'X₂)⁻¹X₂'X₁)]⁻¹[X₁'y - X₁'X₂(X₂'X₂)⁻¹X₂'y]\")\n",
    "    println()\n",
    "    \n",
    "    println(\"Step 3: Factoring out the projection matrix\")\n",
    "    println(\"Let M_{X₂} = I - X₂(X₂'X₂)⁻¹X₂' (the annihilator matrix)\")\n",
    "    println(\"Note that M_{X₂} is idempotent: M_{X₂}M_{X₂} = M_{X₂}\")\n",
    "    println(\"And symmetric: M_{X₂}' = M_{X₂}\")\n",
    "    println()\n",
    "    println(\"Then:\")\n",
    "    println(\"X₁'X₁ - X₁'X₂(X₂'X₂)⁻¹X₂'X₁ = X₁'[I - X₂(X₂'X₂)⁻¹X₂']X₁ = X₁'M_{X₂}X₁\")\n",
    "    println(\"X₁'y - X₁'X₂(X₂'X₂)⁻¹X₂'y = X₁'[I - X₂(X₂'X₂)⁻¹X₂']y = X₁'M_{X₂}y\")\n",
    "    println()\n",
    "    \n",
    "    println(\"Step 4: Final form\")\n",
    "    println(\"Therefore:\")\n",
    "    println(\"β̂₁ = (X₁'M_{X₂}X₁)⁻¹X₁'M_{X₂}y\")\n",
    "    println()\n",
    "    println(\"Let X̃₁ = M_{X₂}X₁ and ỹ = M_{X₂}y\")\n",
    "    println(\"Then: β̂₁ = (X̃₁'X̃₁)⁻¹X̃₁'ỹ\")\n",
    "    println()\n",
    "    println(\"This shows that β̂₁ from the full regression equals the OLS coefficient\")\n",
    "    println(\"from regressing the residuals ỹ on the residuals X̃₁.\")\n",
    "    println()\n",
    "    println(\"Q.E.D.\")\n",
    "    println()\n",
    "end\n",
    "\n",
    "# Display the mathematical proof\n",
    "fwl_theorem_proof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Verification\n",
    "\n",
    "Now let's verify the FWL theorem numerically using simulated data. We'll generate data with known parameters and compare the results from:\n",
    "1. Full regression: y ~ [X₁ X₂]\n",
    "2. FWL two-step procedure: residuals of y on residuals of X₁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function numerical_verification()\n",
    "    \"\"\"\n",
    "    Numerical verification of the FWL theorem using simulated data.\n",
    "    \"\"\"\n",
    "    println(\"=== NUMERICAL VERIFICATION ===\\n\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    Random.seed!(42)\n",
    "    \n",
    "    # Generate data\n",
    "    n = 1000  # Sample size\n",
    "    k1 = 2    # Number of variables of interest\n",
    "    k2 = 3    # Number of control variables\n",
    "    \n",
    "    # Generate X1, X2, and error term\n",
    "    X1 = randn(n, k1)\n",
    "    X2 = randn(n, k2)\n",
    "    u = randn(n, 1)\n",
    "    \n",
    "    # True parameters\n",
    "    beta1_true = [1.5; 2.0]\n",
    "    beta2_true = [0.5; -1.0; 0.8]\n",
    "    \n",
    "    # Generate y\n",
    "    y = X1 * beta1_true + X2 * beta2_true + u\n",
    "    \n",
    "    @printf(\"Sample size: %d\\n\", n)\n",
    "    @printf(\"X1 dimensions: (%d, %d) (variables of interest)\\n\", size(X1)...)\n",
    "    @printf(\"X2 dimensions: (%d, %d) (control variables)\\n\", size(X2)...)\n",
    "    @printf(\"True β₁: [%.1f, %.1f]\\n\", beta1_true...)\n",
    "    @printf(\"True β₂: [%.1f, %.1f, %.1f]\\n\", beta2_true...)\n",
    "    println()\n",
    "    \n",
    "    return X1, X2, y, n, k1, k2, beta1_true, beta2_true\n",
    "end\n",
    "\n",
    "# Generate the data\n",
    "X1, X2, y, n, k1, k2, beta1_true, beta2_true = numerical_verification();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Full Regression\n",
    "\n",
    "First, let's estimate the full regression model with all variables using Julia's efficient linear algebra operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Full regression\n",
    "X_full = hcat(X1, X2)\n",
    "beta_full = (X_full' * X_full) \\ (X_full' * y)\n",
    "beta1_full = beta_full[1:k1]\n",
    "\n",
    "println(\"Method 1: Full regression\")\n",
    "@printf(\"β̂₁ from full regression: [%.6f, %.6f]\\n\", beta1_full...)\n",
    "println()\n",
    "\n",
    "# Display the full coefficient vector\n",
    "println(\"All coefficients from full regression:\")\n",
    "for i in 1:length(beta_full)\n",
    "    var_name = i <= k1 ? \"β₁[$i]\" : \"β₂[$(i-k1)]\"\n",
    "    @printf(\"%s: %.6f\\n\", var_name, beta_full[i])\n",
    "end\n",
    "println()\n",
    "\n",
    "# Calculate R-squared\n",
    "y_pred_full = X_full * beta_full\n",
    "sst = sum((y .- mean(y)).^2)\n",
    "sse = sum((y .- y_pred_full).^2)\n",
    "r_squared = 1 - sse/sst\n",
    "\n",
    "@printf(\"R-squared: %.4f\\n\", r_squared)\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: FWL Two-Step Procedure\n",
    "\n",
    "Now let's implement the FWL two-step procedure:\n",
    "1. Residualize y and X₁ with respect to X₂\n",
    "2. Regress the residualized y on the residualized X₁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: FWL two-step procedure\n",
    "\n",
    "# Step 1: Regress y on X2 and get residuals\n",
    "P_X2 = X2 * inv(X2' * X2) * X2'\n",
    "M_X2 = I - P_X2\n",
    "y_tilde = M_X2 * y\n",
    "\n",
    "# Step 2: Regress X1 on X2 and get residuals\n",
    "X1_tilde = M_X2 * X1\n",
    "\n",
    "# Step 3: Regress y_tilde on X1_tilde\n",
    "beta1_fwl = (X1_tilde' * X1_tilde) \\ (X1_tilde' * y_tilde)\n",
    "\n",
    "println(\"Method 2: FWL two-step procedure\")\n",
    "println(\"Step 1: Residualize y on X₂\")\n",
    "println(\"Step 2: Residualize X₁ on X₂\")\n",
    "println(\"Step 3: Regress residuals\")\n",
    "@printf(\"β̂₁ from FWL method: [%.6f, %.6f]\\n\", beta1_fwl...)\n",
    "println()\n",
    "\n",
    "# Show some properties of the projection matrices\n",
    "println(\"Properties of projection matrices:\")\n",
    "@printf(\"Rank of P_X2: %d (should equal k2 = %d)\\n\", rank(P_X2), k2)\n",
    "@printf(\"Rank of M_X2: %d (should equal n - k2 = %d)\\n\", rank(M_X2), n - k2)\n",
    "@printf(\"Trace of P_X2: %.0f (should equal k2 = %d)\\n\", tr(P_X2), k2)\n",
    "@printf(\"Trace of M_X2: %.0f (should equal n - k2 = %d)\\n\", tr(M_X2), n - k2)\n",
    "println()\n",
    "\n",
    "# Check idempotency\n",
    "idempotent_P = maximum(abs.(P_X2 * P_X2 - P_X2))\n",
    "idempotent_M = maximum(abs.(M_X2 * M_X2 - M_X2))\n",
    "@printf(\"P_X2 idempotency check (max |P²-P|): %.2e\\n\", idempotent_P)\n",
    "@printf(\"M_X2 idempotency check (max |M²-M|): %.2e\\n\", idempotent_M)\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison and Verification\n",
    "\n",
    "Let's check if both methods produce identical results (within numerical precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if they are equal (within numerical precision)\n",
    "difference = abs.(beta1_full - beta1_fwl)\n",
    "max_diff = maximum(difference)\n",
    "\n",
    "println(\"Verification:\")\n",
    "@printf(\"Maximum absolute difference: %.2e\\n\", max_diff)\n",
    "@printf(\"Are they equal (within 1e-10)? %s\\n\", max_diff < 1e-10)\n",
    "println()\n",
    "\n",
    "# Show element-wise differences\n",
    "println(\"Element-wise differences:\")\n",
    "for i in 1:k1\n",
    "    @printf(\"β₁[%d]: Full = %.8f, FWL = %.8f, Diff = %.2e\\n\", \n",
    "            i, beta1_full[i], beta1_fwl[i], difference[i])\n",
    "end\n",
    "println()\n",
    "\n",
    "# Store results for summary\n",
    "results = Dict(\n",
    "    \"beta1_full\" => beta1_full,\n",
    "    \"beta1_fwl\" => beta1_fwl,\n",
    "    \"max_difference\" => max_diff,\n",
    "    \"r_squared\" => r_squared\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Let's create some plots to visualize the relationship between the original and residualized variables using Julia's Plots.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "p1 = scatter(X1[:, 1], y[:, 1], alpha=0.6, color=:blue, \n",
    "            xlabel=\"X1[,1]\", ylabel=\"y\", \n",
    "            title=\"Original Data: y vs X1[,1]\",\n",
    "            markersize=2, legend=false)\n",
    "\n",
    "p2 = scatter(X1_tilde[:, 1], y_tilde[:, 1], alpha=0.6, color=:green,\n",
    "            xlabel=\"X1_tilde[,1]\", ylabel=\"y_tilde\", \n",
    "            title=\"Residualized Data: y_tilde vs X1_tilde[,1]\",\n",
    "            markersize=2, legend=false)\n",
    "\n",
    "p3 = scatter(X1[:, 2], y[:, 1], alpha=0.6, color=:blue,\n",
    "            xlabel=\"X1[,2]\", ylabel=\"y\", \n",
    "            title=\"Original Data: y vs X1[,2]\",\n",
    "            markersize=2, legend=false)\n",
    "\n",
    "p4 = scatter(X1_tilde[:, 2], y_tilde[:, 1], alpha=0.6, color=:green,\n",
    "            xlabel=\"X1_tilde[,2]\", ylabel=\"y_tilde\", \n",
    "            title=\"Residualized Data: y_tilde vs X1_tilde[,2]\",\n",
    "            markersize=2, legend=false)\n",
    "\n",
    "# Combine plots\n",
    "plot(p1, p2, p3, p4, layout=(2,2), size=(800, 600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Properties Verification\n",
    "\n",
    "Let's verify some important properties of projection matrices in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify projection matrix properties\n",
    "println(\"=== PROJECTION MATRIX PROPERTIES ===\")\n",
    "println()\n",
    "\n",
    "# 1. Symmetry\n",
    "P_symmetric = maximum(abs.(P_X2 - P_X2'))\n",
    "M_symmetric = maximum(abs.(M_X2 - M_X2'))\n",
    "@printf(\"P_X2 symmetry check (max |P-P'|): %.2e\\n\", P_symmetric)\n",
    "@printf(\"M_X2 symmetry check (max |M-M'|): %.2e\\n\", M_symmetric)\n",
    "\n",
    "# 2. Idempotency (already checked above)\n",
    "@printf(\"P_X2 idempotency check (max |P²-P|): %.2e\\n\", idempotent_P)\n",
    "@printf(\"M_X2 idempotency check (max |M²-M|): %.2e\\n\", idempotent_M)\n",
    "\n",
    "# 3. Complementarity\n",
    "complementary = maximum(abs.(P_X2 + M_X2 - I))\n",
    "@printf(\"Complementarity check (max |P+M-I|): %.2e\\n\", complementary)\n",
    "\n",
    "# 4. Orthogonality\n",
    "orthogonal = maximum(abs.(P_X2 * M_X2))\n",
    "@printf(\"Orthogonality check (max |PM|): %.2e\\n\", orthogonal)\n",
    "\n",
    "# 5. Eigenvalue properties\n",
    "P_eigenvals = eigvals(P_X2)\n",
    "M_eigenvals = eigvals(M_X2)\n",
    "\n",
    "println(\"\\nEigenvalue properties:\")\n",
    "@printf(\"P_X2 eigenvalues close to 0 or 1: %s\\n\", \n",
    "        all(abs.(P_eigenvals) .< 1e-10 .|| abs.(P_eigenvals .- 1) .< 1e-10))\n",
    "@printf(\"M_X2 eigenvalues close to 0 or 1: %s\\n\", \n",
    "        all(abs.(M_eigenvals) .< 1e-10 .|| abs.(M_eigenvals .- 1) .< 1e-10))\n",
    "\n",
    "println(\"\\n✅ All projection matrix properties verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Results\n",
    "\n",
    "Let's create a comprehensive summary table of all our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary\n",
    "println(\"\\n=== RESULTS SUMMARY ===\")\n",
    "println()\n",
    "\n",
    "println(\"Method Comparison:\")\n",
    "println(\"------------------\")\n",
    "@printf(\"%-25s %12s %12s\\n\", \"Method\", \"β₁[1]\", \"β₁[2]\")\n",
    "println(\"-\" ^ 49)\n",
    "@printf(\"%-25s %12.6f %12.6f\\n\", \"True Values\", beta1_true...)\n",
    "@printf(\"%-25s %12.6f %12.6f\\n\", \"Full Regression\", beta1_full...)\n",
    "@printf(\"%-25s %12.6f %12.6f\\n\", \"FWL Method\", beta1_fwl...)\n",
    "\n",
    "println(\"\\nVerification Statistics:\")\n",
    "println(\"------------------------\")\n",
    "@printf(\"Maximum difference: %.2e\\n\", results[\"max_difference\"])\n",
    "@printf(\"Methods match (within 1e-10): %s\\n\", results[\"max_difference\"] < 1e-10)\n",
    "@printf(\"R-squared: %.4f\\n\", results[\"r_squared\"])\n",
    "\n",
    "println(\"\\nMatrix Dimensions:\")\n",
    "println(\"------------------\")\n",
    "@printf(\"Sample size (n): %d\\n\", n)\n",
    "@printf(\"Variables of interest (k1): %d\\n\", k1)\n",
    "@printf(\"Control variables (k2): %d\\n\", k2)\n",
    "@printf(\"X1 dimensions: %s\\n\", size(X1))\n",
    "@printf(\"X2 dimensions: %s\\n\", size(X2))\n",
    "@printf(\"y dimensions: %s\\n\", size(y))\n",
    "\n",
    "println(\"\\n✅ FWL Theorem verification SUCCESSFUL!\")\n",
    "println(\"The full regression and FWL two-step procedure produce identical results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "\n",
    "Let's compare the computational performance of both methods using Julia's `@time` macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "println(\"=== PERFORMANCE COMPARISON ===\")\n",
    "println()\n",
    "\n",
    "println(\"Timing full regression method:\")\n",
    "@time begin\n",
    "    X_full_perf = hcat(X1, X2)\n",
    "    beta_full_perf = (X_full_perf' * X_full_perf) \\ (X_full_perf' * y)\n",
    "    beta1_full_perf = beta_full_perf[1:k1]\n",
    "end\n",
    "\n",
    "println(\"\\nTiming FWL method:\")\n",
    "@time begin\n",
    "    P_X2_perf = X2 * inv(X2' * X2) * X2'\n",
    "    M_X2_perf = I - P_X2_perf\n",
    "    y_tilde_perf = M_X2_perf * y\n",
    "    X1_tilde_perf = M_X2_perf * X1\n",
    "    beta1_fwl_perf = (X1_tilde_perf' * X1_tilde_perf) \\ (X1_tilde_perf' * y_tilde_perf)\n",
    "end\n",
    "\n",
    "println(\"\\nNote: FWL method is more computationally expensive for this simple case,\")\n",
    "println(\"but becomes advantageous in high-dimensional settings or when computing\")\n",
    "println(\"coefficients for subsets of variables repeatedly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have successfully:\n",
    "\n",
    "1. **Provided a complete mathematical proof** of the Frisch-Waugh-Lovell theorem using partitioned matrix algebra\n",
    "2. **Numerically verified** the theorem using simulated data with Julia's efficient linear algebra operations\n",
    "3. **Demonstrated** that both the full regression and the FWL two-step procedure produce identical estimates (within machine precision)\n",
    "4. **Verified projection matrix properties** including symmetry, idempotency, and complementarity\n",
    "5. **Visualized** the relationship between original and residualized variables\n",
    "6. **Compared computational performance** of both approaches\n",
    "\n",
    "### Julia-Specific Advantages:\n",
    "- **Performance**: Julia's just-in-time compilation provides near-C performance for numerical operations\n",
    "- **Syntax**: Clean, mathematical notation that closely resembles theoretical formulations\n",
    "- **Linear Algebra**: Built-in support for advanced linear algebra operations\n",
    "- **Memory Efficiency**: Efficient memory management for large matrices\n",
    "- **Ecosystem**: Rich ecosystem of packages for statistics and econometrics\n",
    "\n",
    "The FWL theorem is a powerful tool in econometrics that allows us to:\n",
    "- Isolate the effect of specific variables by \"partialling out\" control variables\n",
    "- Understand the mechanics of multiple regression\n",
    "- Implement efficient computational methods for large datasets\n",
    "- Gain intuition about what multiple regression coefficients actually measure\n",
    "\n",
    "### Key Insights:\n",
    "- The projection matrix M_{X₂} removes the linear association with control variables\n",
    "- The residualized variables contain only the variation orthogonal to the controls\n",
    "- The FWL coefficient captures the relationship between y and X₁ after \"controlling for\" X₂\n",
    "- This provides the theoretical foundation for interpreting multiple regression coefficients\n",
    "\n",
    "**This completes Part 1 of Assignment 1 in Julia.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}