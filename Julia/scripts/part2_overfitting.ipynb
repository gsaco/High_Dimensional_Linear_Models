{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Part 2: Overfitting Analysis\n",
    "## Overfitting (8 points)\n",
    "\n",
    "This notebook analyzes overfitting using a procedure similar to simulation.ipynb. We use a simple data generating process and study how R-squared measures change with model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Random\n",
    "using Printf\n",
    "using Plots\n",
    "using DataFrames\n",
    "using CSV\n",
    "using Statistics\n",
    "\n",
    "# Set plotting backend\n",
    "gr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Following the simulation.ipynb approach, we generate data with a convenient slope (PGD) for all three languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_data(n=1000; seed=42)\n",
    "    \"\"\"\n",
    "    Generate data following the specification similar to simulation.ipynb.\n",
    "    Two variables X and Y, intercept parameter is zero.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n : Int\n",
    "        Sample size (default: 1000)\n",
    "    seed : Int\n",
    "        Random seed for reproducibility (42)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : Matrix\n",
    "        Feature matrix (n x 1)\n",
    "    y : Vector\n",
    "        Target variable (n,)\n",
    "    \"\"\"\n",
    "    Random.seed!(seed)\n",
    "    \n",
    "    # Generate X from uniform distribution like in simulation.ipynb\n",
    "    X_raw = rand(n)\n",
    "    X_raw = sort(X_raw)  # Sort like in simulation\n",
    "    X = reshape(X_raw, n, 1)\n",
    "    \n",
    "    # Generate error term\n",
    "    e = randn(n)\n",
    "    \n",
    "    # Generate y with no intercept (as requested)\n",
    "    # True relationship: y = 2*X + e (convenient slope for all languages)\n",
    "    beta_true = 2.0\n",
    "    y = beta_true * X[:, 1] + e\n",
    "    \n",
    "    return X, y\n",
    "end\n",
    "\n",
    "# Generate the data\n",
    "X, y = generate_data(1000, seed=42)\n",
    "\n",
    "@printf(\"Generated data with n=%d observations\\n\", length(y))\n",
    "println(\"True relationship: y = 2*X + e (convenient slope = 2.0)\")\n",
    "@printf(\"X range: [%.4f, %.4f]\\n\", minimum(X), maximum(X))\n",
    "@printf(\"y range: [%.4f, %.4f]\\n\", minimum(y), maximum(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_polynomial_features(X, n_features)\n",
    "    \"\"\"\n",
    "    Create polynomial features up to n_features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : Matrix\n",
    "        Original feature matrix (n x 1)\n",
    "    n_features : Int\n",
    "        Number of features to create\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_poly : Matrix\n",
    "        Extended feature matrix with polynomial features\n",
    "    \"\"\"\n",
    "    n_samples = size(X, 1)\n",
    "    X_poly = zeros(n_samples, n_features)\n",
    "    \n",
    "    for i in 1:n_features\n",
    "        X_poly[:, i] = X[:, 1] .^ i  # x^1, x^2, x^3, etc.\n",
    "    end\n",
    "    \n",
    "    return X_poly\n",
    "end\n",
    "\n",
    "function calculate_adjusted_r2(r2, n, k)\n",
    "    \"\"\"\n",
    "    Calculate adjusted R-squared.\n",
    "    \n",
    "    Adjusted R¬≤ = 1 - [(1 - R¬≤)(n - 1) / (n - k - 1)]\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    r2 : Float64\n",
    "        R-squared value\n",
    "    n : Int\n",
    "        Sample size\n",
    "    k : Int\n",
    "        Number of features (excluding intercept)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    adj_r2 : Float64\n",
    "        Adjusted R-squared\n",
    "    \"\"\"\n",
    "    if n - k - 1 <= 0\n",
    "        return NaN\n",
    "    end\n",
    "    \n",
    "    adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "    return adj_r2\n",
    "end\n",
    "\n",
    "function r2_score(y_true, y_pred)\n",
    "    \"\"\"Calculate R-squared score.\"\"\"\n",
    "    ss_res = sum((y_true - y_pred).^2)\n",
    "    ss_tot = sum((y_true .- mean(y_true)).^2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "end\n",
    "\n",
    "function train_test_split(X, y; test_size=0.25, random_state=42)\n",
    "    \"\"\"Split data into training and testing sets.\"\"\"\n",
    "    Random.seed!(random_state)\n",
    "    n = length(y)\n",
    "    n_test = round(Int, n * test_size)\n",
    "    indices = randperm(n)\n",
    "    \n",
    "    test_indices = indices[1:n_test]\n",
    "    train_indices = indices[n_test+1:end]\n",
    "    \n",
    "    return X[train_indices, :], X[test_indices, :], y[train_indices], y[test_indices]\n",
    "end\n",
    "\n",
    "# Test the functions\n",
    "X_poly_example = create_polynomial_features(X, 5)\n",
    "@printf(\"Original X shape: (%d, %d)\\n\", size(X)...)\n",
    "@printf(\"Polynomial features (5 features) shape: (%d, %d)\\n\", size(X_poly_example)...)\n",
    "@printf(\"Example adjusted R¬≤: %.4f\\n\", calculate_adjusted_r2(0.8, 1000, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Analysis\n",
    "\n",
    "Test models with different numbers of polynomial features: 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function overfitting_analysis()\n",
    "    \"\"\"\n",
    "    Main function to perform overfitting analysis.\n",
    "    \"\"\"\n",
    "    println(\"=== OVERFITTING ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Number of features to test (as specified)\n",
    "    n_features_list = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "    \n",
    "    # Storage for results\n",
    "    results = DataFrame(\n",
    "        n_features = Int[],\n",
    "        r2_full = Float64[],\n",
    "        adj_r2_full = Float64[],\n",
    "        r2_out_of_sample = Float64[]\n",
    "    )\n",
    "    \n",
    "    println(\"Analyzing overfitting for different numbers of features...\")\n",
    "    println(\"Features | R¬≤ (full) | Adj R¬≤ (full) | R¬≤ (out-of-sample)\")\n",
    "    println(\"-\" ^ 60)\n",
    "    \n",
    "    for n_feat in n_features_list\n",
    "        try\n",
    "            # Create polynomial features\n",
    "            X_poly = create_polynomial_features(X, n_feat)\n",
    "            \n",
    "            # Split data into train/test (75%/25%)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.25, random_state=42)\n",
    "            \n",
    "            # Fit model on full sample (no intercept as requested)\n",
    "            # Using linear algebra: beta = (X'X)^(-1) X'y\n",
    "            if n_feat >= size(X_poly, 1)\n",
    "                # When n_features >= n_samples, use regularized solution\n",
    "                lambda = 1e-6\n",
    "                beta_full = (X_poly' * X_poly + lambda * I) \\ (X_poly' * y)\n",
    "            else\n",
    "                beta_full = (X_poly' * X_poly) \\ (X_poly' * y)\n",
    "            end\n",
    "            y_pred_full = X_poly * beta_full\n",
    "            r2_full = r2_score(y, y_pred_full)\n",
    "            \n",
    "            # Calculate adjusted R¬≤\n",
    "            adj_r2_full = calculate_adjusted_r2(r2_full, length(y), n_feat)\n",
    "            \n",
    "            # Fit model on training data and predict on test data\n",
    "            if n_feat >= size(X_train, 1)\n",
    "                # When n_features >= n_samples, use regularized solution\n",
    "                lambda = 1e-6\n",
    "                beta_train = (X_train' * X_train + lambda * I) \\ (X_train' * y_train)\n",
    "            else\n",
    "                beta_train = (X_train' * X_train) \\ (X_train' * y_train)\n",
    "            end\n",
    "            y_pred_test = X_test * beta_train\n",
    "            r2_out_of_sample = r2_score(y_test, y_pred_test)\n",
    "            \n",
    "            # Store results\n",
    "            push!(results, (n_feat, r2_full, adj_r2_full, r2_out_of_sample))\n",
    "            \n",
    "            @printf(\"%8d | %9.4f | %12.4f | %17.4f\\n\", n_feat, r2_full, adj_r2_full, r2_out_of_sample)\n",
    "            \n",
    "        catch e\n",
    "            println(\"Error with $n_feat features: $e\")\n",
    "            # Still append to maintain consistency\n",
    "            push!(results, (n_feat, NaN, NaN, NaN))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    println()\n",
    "    return results\n",
    "end\n",
    "\n",
    "# Run the analysis\n",
    "results_df = overfitting_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Create three separate graphs for each R-squared measure as requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_separate_plots(df_results)\n",
    "    \"\"\"\n",
    "    Create three separate plots for R-squared analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_results : DataFrame\n",
    "        Results from overfitting analysis\n",
    "    \"\"\"\n",
    "    println(\"Creating plots...\")\n",
    "    \n",
    "    # Filter out NaN values\n",
    "    valid_rows = completecases(df_results)\n",
    "    df_clean = df_results[valid_rows, :]\n",
    "    \n",
    "    # Plot 1: R-squared (full sample)\n",
    "    p1 = plot(df_clean.n_features, df_clean.r2_full,\n",
    "              marker=:circle, linewidth=2, markersize=6, color=:blue,\n",
    "              title=\"R-squared on Full Sample vs Number of Features\",\n",
    "              xlabel=\"Number of Features\", ylabel=\"R-squared\",\n",
    "              xscale=:log10, ylims=(0, 1), grid=true,\n",
    "              titlefontsize=12, labelfontsize=10,\n",
    "              legend=false)\n",
    "    \n",
    "    display(p1)\n",
    "    \n",
    "    # Plot 2: Adjusted R-squared (full sample)  \n",
    "    p2 = plot(df_clean.n_features, df_clean.adj_r2_full,\n",
    "              marker=:square, linewidth=2, markersize=6, color=:green,\n",
    "              title=\"Adjusted R-squared on Full Sample vs Number of Features\",\n",
    "              xlabel=\"Number of Features\", ylabel=\"Adjusted R-squared\",\n",
    "              xscale=:log10, grid=true,\n",
    "              titlefontsize=12, labelfontsize=10,\n",
    "              legend=false)\n",
    "    \n",
    "    display(p2)\n",
    "    \n",
    "    # Plot 3: Out-of-sample R-squared\n",
    "    p3 = plot(df_clean.n_features, df_clean.r2_out_of_sample,\n",
    "              marker=:utriangle, linewidth=2, markersize=6, color=:red,\n",
    "              title=\"Out-of-Sample R-squared vs Number of Features\",\n",
    "              xlabel=\"Number of Features\", ylabel=\"Out-of-Sample R-squared\",\n",
    "              xscale=:log10, grid=true,\n",
    "              titlefontsize=12, labelfontsize=10,\n",
    "              legend=false)\n",
    "    \n",
    "    display(p3)\n",
    "    \n",
    "    println(\"Plots created successfully!\")\n",
    "    \n",
    "    return p1, p2, p3\n",
    "end\n",
    "\n",
    "# Create the plots\n",
    "p1, p2, p3 = create_separate_plots(results_df)\n",
    "\n",
    "println(\"\\nThree separate plots created showing:\")\n",
    "println(\"1. R¬≤ (Full Sample): Shows monotonic increase\")\n",
    "println(\"2. Adjusted R¬≤ (Full Sample): Shows peak and decline due to complexity penalty\")\n",
    "println(\"3. R¬≤ (Out-of-Sample): Shows the classic overfitting pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display complete results\n",
    "println(\"\\n=== COMPLETE RESULTS TABLE ===\")\n",
    "println(results_df)\n",
    "\n",
    "# Find optimal complexity\n",
    "valid_results = results_df[completecases(results_df), :]\n",
    "if nrow(valid_results) > 0\n",
    "    optimal_adj_r2_idx = argmax(valid_results.adj_r2_full)\n",
    "    optimal_oos_r2_idx = argmax(valid_results.r2_out_of_sample)\n",
    "    \n",
    "    println(\"\\n=== OPTIMAL MODEL COMPLEXITY ===\")\n",
    "    @printf(\"By Adjusted R¬≤: %d features\\n\", valid_results.n_features[optimal_adj_r2_idx])\n",
    "    @printf(\"By Out-of-Sample R¬≤: %d features\\n\", valid_results.n_features[optimal_oos_r2_idx])\n",
    "end\n",
    "\n",
    "println(\"\\n=== INSIGHTS ===\")\n",
    "println(\"‚úÖ This analysis demonstrates the classic bias-variance tradeoff\")\n",
    "println(\"üìà R¬≤ (Full Sample) increases monotonically with model complexity\")\n",
    "println(\"üìä Adjusted R¬≤ peaks early and then declines due to complexity penalty\")\n",
    "println(\"üìâ Out-of-Sample R¬≤ shows the inverted U-shape characteristic of overfitting\")\n",
    "println(\"üéØ True model has only 1 feature (y = 2*X + e), but polynomial terms can help initially\")\n",
    "println(\"‚ö†Ô∏è High-dimensional models (many features) lead to severe overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory and save results\n",
    "output_dir = \"../output\"\n",
    "mkpath(output_dir)\n",
    "\n",
    "# Save results\n",
    "CSV.write(joinpath(output_dir, \"overfitting_results_julia.csv\"), results_df)\n",
    "println(\"Results saved to $(output_dir)/overfitting_results_julia.csv\")\n",
    "\n",
    "println(\"\\nüéâ Julia overfitting analysis complete!\")\n",
    "println(\"Data generation follows simulation.ipynb approach with:\")\n",
    "println(\"- X ~ Uniform(0,1), sorted, n=1000\")\n",
    "println(\"- e ~ Normal(0,1)\")\n",
    "println(\"- y = 2*X + e (convenient slope = 2.0)\")\n",
    "println(\"- No intercept (as requested)\")\n",
    "println(\"- Seed = 42 for reproducibility\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}