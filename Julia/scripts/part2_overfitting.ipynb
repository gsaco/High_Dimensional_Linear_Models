{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Part 2: Overfitting Analysis\n",
    "## Overfitting (8 points)\n",
    "\n",
    "This notebook simulates a data generating process and analyzes overfitting by estimating linear models with increasing numbers of polynomial features using Julia.\n",
    "\n",
    "We will demonstrate the classic bias-variance tradeoff by examining how different R-squared measures behave as model complexity increases.\n",
    "\n",
    "Julia's performance advantages make it particularly well-suited for this type of computational analysis involving large matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Random\n",
    "using Printf\n",
    "using Plots\n",
    "using DataFrames\n",
    "using CSV\n",
    "using Statistics\n",
    "\n",
    "# Set plotting backend\n",
    "gr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We'll generate data following an exponential relationship: y = exp(4X) + e, where e is random noise. X is generated from a uniform distribution [0,1] and sorted, while e follows a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_data(n=1000; seed=42)\n",
    "    \"\"\"\n",
    "    Generate data following the specification with only 2 variables X and Y.\n",
    "    Uses the new PGD: y = exp(4*X) + e\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n : Int\n",
    "        Sample size (default: 1000)\n",
    "    seed : Int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : Matrix\n",
    "        Feature matrix\n",
    "    y : Vector\n",
    "        Target variable\n",
    "    e : Matrix\n",
    "        Error term\n",
    "    \"\"\"\n",
    "    Random.seed!(seed)\n",
    "    \n",
    "    # Generate X using uniform distribution [0,1], sorted\n",
    "    X = rand(n)\n",
    "    X = sort(X)\n",
    "    X = reshape(X, n, 1)\n",
    "    \n",
    "    # Generate error term e using normal distribution\n",
    "    e = randn(n)\n",
    "    e = reshape(e, n, 1)\n",
    "    \n",
    "    # Generate y using the new PGD: y = exp(4*X) + e\n",
    "    y = exp.(4 * X[:, 1]) + e[:, 1]\n",
    "    \n",
    "    return X, y, e\n",
    "end\n",
    "\n",
    "# Generate the data\n",
    "X, y, e = generate_data(1000, seed=42)\n",
    "\n",
    "@printf(\"Generated data with n=%d observations\\n\", length(y))\n",
    "println(\"True relationship: y = exp(4*X) + e\")\n",
    "@printf(\"X shape: (%d, %d)\\n\", size(X)...)\n",
    "@printf(\"y length: %d\\n\", length(y))\n",
    "@printf(\"X mean: %.4f, X std: %.4f\\n\", mean(X), std(X))\n",
    "@printf(\"y mean: %.4f, y std: %.4f\\n\", mean(y), std(y))\n",
    "println(\"e sample (first 5):\")\n",
    "for i in 1:5\n",
    "    @printf(\"%.4f \", e[i, 1])\n",
    "end\n",
    "println()\n",
    "\n",
    "# Display first few observations\n",
    "println(\"\\nFirst 10 observations:\")\n",
    "for i in 1:10\n",
    "    @printf(\"X[%d] = %7.4f, y[%d] = %7.4f\\n\", i, X[i, 1], i, y[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's define helper functions for polynomial feature creation, adjusted R-squared calculation, and data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_polynomial_features(X, n_features)\n",
    "    \"\"\"\n",
    "    Create polynomial features up to n_features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : Matrix\n",
    "        Original feature matrix (n x 1)\n",
    "    n_features : Int\n",
    "        Number of features to create\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_poly : Matrix\n",
    "        Extended feature matrix with polynomial features\n",
    "    \"\"\"\n",
    "    n_samples = size(X, 1)\n",
    "    X_poly = zeros(n_samples, n_features)\n",
    "    \n",
    "    for i in 1:n_features\n",
    "        X_poly[:, i] = X[:, 1] .^ i  # x^1, x^2, x^3, etc.\n",
    "    end\n",
    "    \n",
    "    return X_poly\n",
    "end\n",
    "\n",
    "function calculate_adjusted_r2(r2, n, k)\n",
    "    \"\"\"\n",
    "    Calculate adjusted R-squared.\n",
    "    \n",
    "    Adjusted R¬≤ = 1 - [(1 - R¬≤)(n - 1) / (n - k - 1)]\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    r2 : Float64\n",
    "        R-squared value\n",
    "    n : Int\n",
    "        Sample size\n",
    "    k : Int\n",
    "        Number of features (excluding intercept)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    adj_r2 : Float64\n",
    "        Adjusted R-squared\n",
    "    \"\"\"\n",
    "    if n - k - 1 <= 0\n",
    "        return NaN\n",
    "    end\n",
    "    \n",
    "    adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "    return adj_r2\n",
    "end\n",
    "\n",
    "function r2_score(y_true, y_pred)\n",
    "    \"\"\"Calculate R-squared score.\"\"\"\n",
    "    ss_res = sum((y_true - y_pred).^2)\n",
    "    ss_tot = sum((y_true .- mean(y_true)).^2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "end\n",
    "\n",
    "function train_test_split(X, y; test_size=0.25, random_state=42)\n",
    "    \"\"\"Split data into training and testing sets.\"\"\"\n",
    "    Random.seed!(random_state)\n",
    "    n = length(y)\n",
    "    n_test = round(Int, n * test_size)\n",
    "    indices = randperm(n)\n",
    "    \n",
    "    test_indices = indices[1:n_test]\n",
    "    train_indices = indices[n_test+1:end]\n",
    "    \n",
    "    return X[train_indices, :], X[test_indices, :], y[train_indices], y[test_indices]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: create polynomial features\n",
    "X_poly_example = create_polynomial_features(X, 5)\n",
    "@printf(\"Example: Original X shape: (%d, %d)\\n\", size(X)...)\n",
    "@printf(\"Example: Polynomial features (5 features) shape: (%d, %d)\\n\", size(X_poly_example)...)\n",
    "println(\"First 5 rows of polynomial features:\")\n",
    "display(X_poly_example[1:5, :])\n",
    "\n",
    "# Example: adjusted R-squared calculation\n",
    "example_r2 = 0.8\n",
    "example_n = 1000\n",
    "example_k = 5\n",
    "example_adj_r2 = calculate_adjusted_r2(example_r2, example_n, example_k)\n",
    "@printf(\"\\nExample: R¬≤ = %.1f, n = %d, k = %d\\n\", example_r2, example_n, example_k)\n",
    "@printf(\"Adjusted R¬≤ = %.4f\\n\", example_adj_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Analysis\n",
    "\n",
    "Now we'll perform the main analysis, testing models with different numbers of polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function overfitting_analysis()\n",
    "    \"\"\"\n",
    "    Main function to perform overfitting analysis.\n",
    "    \"\"\"\n",
    "    println(\"=== OVERFITTING ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Number of features to test\n",
    "    n_features_list = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "    \n",
    "    # Storage for results\n",
    "    results = DataFrame(\n",
    "        n_features = Int[],\n",
    "        r2_full = Float64[],\n",
    "        adj_r2_full = Float64[],\n",
    "        r2_out_of_sample = Float64[]\n",
    "    )\n",
    "    \n",
    "    println(\"Analyzing overfitting for different numbers of features...\")\n",
    "    println(\"Features | R¬≤ (full) | Adj R¬≤ (full) | R¬≤ (out-of-sample)\")\n",
    "    println(\"-\" * 60)\n",
    "    \n",
    "    for n_feat in n_features_list\n",
    "        try\n",
    "            # Create polynomial features\n",
    "            X_poly = create_polynomial_features(X, n_feat)\n",
    "            \n",
    "            # Split data into train/test (75%/25%)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.25, random_state=42)\n",
    "            \n",
    "            # Fit model on full sample (no intercept as requested)\n",
    "            beta_full = (X_poly' * X_poly) \\ (X_poly' * y)\n",
    "            y_pred_full = X_poly * beta_full\n",
    "            r2_full = r2_score(y, y_pred_full)\n",
    "            \n",
    "            # Calculate adjusted R¬≤\n",
    "            adj_r2_full = calculate_adjusted_r2(r2_full, length(y), n_feat)\n",
    "            \n",
    "            # Fit model on training data and predict on test data\n",
    "            beta_train = (X_train' * X_train) \\ (X_train' * y_train)\n",
    "            y_pred_test = X_test * beta_train\n",
    "            r2_out_of_sample = r2_score(y_test, y_pred_test)\n",
    "            \n",
    "            # Store results\n",
    "            push!(results, (n_feat, r2_full, adj_r2_full, r2_out_of_sample))\n",
    "            \n",
    "            @printf(\"%8d | %9.4f | %12.4f | %17.4f\\n\", n_feat, r2_full, adj_r2_full, r2_out_of_sample)\n",
    "            \n",
    "        catch e\n",
    "            println(\"Error with $n_feat features: $e\")\n",
    "            # Still append to maintain list length\n",
    "            push!(results, (n_feat, NaN, NaN, NaN))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    println()\n",
    "    return results\n",
    "end\n",
    "\n",
    "# Run the analysis\n",
    "results_df = overfitting_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's create plots to visualize the different R-squared measures as a function of model complexity using Julia's Plots.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_plots(df_results)\n",
    "    \"\"\"\n",
    "    Create three separate plots for R-squared analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_results : DataFrame\n",
    "        Results from overfitting analysis\n",
    "    \"\"\"\n",
    "    println(\"Creating plots...\")\n",
    "    \n",
    "    # Plot 1: R-squared (full sample)\n",
    "    p1 = plot(df_results.n_features, df_results.r2_full,\n",
    "              marker=:circle, linewidth=2, markersize=6, color=:blue,\n",
    "              title=\"R-squared on Full Sample vs Number of Features\",\n",
    "              xlabel=\"Number of Features\", ylabel=\"R-squared\",\n",
    "              xscale=:log10, ylims=(0, 1), grid=true,\n",
    "              titlefontsize=12, labelfontsize=10,\n",
    "              legend=false)\n",
    "    \n",
    "    display(p1)\n",
    "    \n",
    "    # Plot 2: Adjusted R-squared (full sample)  \n",
    "    p2 = plot(df_results.n_features, df_results.adj_r2_full,\n",
    "              marker=:square, linewidth=2, markersize=6, color=:green,\n",
    "              title=\"Adjusted R-squared on Full Sample vs Number of Features\",\n",
    "              xlabel=\"Number of Features\", ylabel=\"Adjusted R-squared\",\n",
    "              xscale=:log10, grid=true,\n",
    "              titlefontsize=12, labelfontsize=10,\n",
    "              legend=false)\n",
    "    \n",
    "    display(p2)\n",
    "    \n",
    "    # Plot 3: Out-of-sample R-squared\n",
    "    p3 = plot(df_results.n_features, df_results.r2_out_of_sample,\n",
    "              marker=:utriangle, linewidth=2, markersize=6, color=:red,\n",
    "              title=\"Out-of-Sample R-squared vs Number of Features\",\n",
    "              xlabel=\"Number of Features\", ylabel=\"Out-of-Sample R-squared\",\n",
    "              xscale=:log10, grid=true,\n",
    "              titlefontsize=12, labelfontsize=10,\n",
    "              legend=false)\n",
    "    \n",
    "    display(p3)\n",
    "    \n",
    "    println(\"Plots created successfully!\")\n",
    "    \n",
    "    return p1, p2, p3\n",
    "end\n",
    "\n",
    "# Create the plots\n",
    "p1, p2, p3 = create_plots(results_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Visualization\n",
    "\n",
    "Let's create a combined plot showing all three R-squared measures for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined plot\n",
    "p_combined = plot()\n",
    "\n",
    "# Add each series\n",
    "plot!(p_combined, results_df.n_features, results_df.r2_full,\n",
    "      marker=:circle, linewidth=2, markersize=4, color=:blue,\n",
    "      label=\"R¬≤ (Full Sample)\", xscale=:log10)\n",
    "\n",
    "plot!(p_combined, results_df.n_features, results_df.adj_r2_full,\n",
    "      marker=:square, linewidth=2, markersize=4, color=:green,\n",
    "      label=\"Adjusted R¬≤ (Full Sample)\")\n",
    "\n",
    "plot!(p_combined, results_df.n_features, results_df.r2_out_of_sample,\n",
    "      marker=:utriangle, linewidth=2, markersize=4, color=:red,\n",
    "      label=\"R¬≤ (Out-of-Sample)\")\n",
    "\n",
    "plot!(p_combined, title=\"Comparison of R-squared Measures vs Number of Features\",\n",
    "      xlabel=\"Number of Features (log scale)\", ylabel=\"R-squared Value\",\n",
    "      grid=true, titlefontsize=14, labelfontsize=12,\n",
    "      legendfontsize=10, legend=:right)\n",
    "\n",
    "display(p_combined)\n",
    "\n",
    "println(\"\\nCombined plot shows all three R-squared measures for easy comparison.\")\n",
    "println(\"Notice how they diverge as model complexity increases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Interpretation\n",
    "\n",
    "Let's analyze the patterns we observe and understand the economic intuition behind them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function interpret_results(df_results)\n",
    "    \"\"\"\n",
    "    Provide interpretation and intuition for the results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_results : DataFrame\n",
    "        Results from overfitting analysis\n",
    "    \"\"\"\n",
    "    println(\"\\n=== RESULTS INTERPRETATION ===\\n\")\n",
    "    \n",
    "    println(\"Key Observations:\")\n",
    "    println(\"================\")\n",
    "    \n",
    "    # R-squared observations\n",
    "    max_r2_full = maximum(df_results.r2_full[.!isnan.(df_results.r2_full)])\n",
    "    max_r2_idx = findfirst(x -> x == max_r2_full, df_results.r2_full)\n",
    "    max_r2_features = df_results.n_features[max_r2_idx]\n",
    "    \n",
    "    @printf(\"1. R-squared (Full Sample):\\n\")\n",
    "    @printf(\"   - Starts at %.4f with 1 feature\\n\", df_results.r2_full[1])\n",
    "    @printf(\"   - Reaches maximum of %.4f with %d features\\n\", max_r2_full, max_r2_features)\n",
    "    @printf(\"   - Shows monotonic increase as expected in in-sample fit\\n\")\n",
    "    println()\n",
    "    \n",
    "    # Adjusted R-squared observations\n",
    "    valid_adj_r2 = df_results.adj_r2_full[.!isnan.(df_results.adj_r2_full)]\n",
    "    interpretation_results = Dict()\n",
    "    \n",
    "    if !isempty(valid_adj_r2)\n",
    "        max_adj_r2 = maximum(valid_adj_r2)\n",
    "        max_adj_r2_idx = findfirst(x -> x == max_adj_r2, df_results.adj_r2_full)\n",
    "        max_adj_r2_features = df_results.n_features[max_adj_r2_idx]\n",
    "        \n",
    "        @printf(\"2. Adjusted R-squared (Full Sample):\\n\")\n",
    "        @printf(\"   - Peaks at %.4f with %d features\\n\", max_adj_r2, max_adj_r2_features)\n",
    "        @printf(\"   - Then declines as the penalty for additional features outweighs benefit\\n\")\n",
    "        @printf(\"   - Becomes negative when model is severely overfitted\\n\")\n",
    "        println()\n",
    "        \n",
    "        interpretation_results[\"max_adj_r2\"] = max_adj_r2\n",
    "        interpretation_results[\"optimal_features_adj_r2\"] = max_adj_r2_features\n",
    "    end\n",
    "    \n",
    "    # Out-of-sample observations\n",
    "    valid_oos_r2 = df_results.r2_out_of_sample[.!isnan.(df_results.r2_out_of_sample)]\n",
    "    if !isempty(valid_oos_r2)\n",
    "        max_oos_r2 = maximum(valid_oos_r2)\n",
    "        max_oos_r2_idx = findfirst(x -> x == max_oos_r2, df_results.r2_out_of_sample)\n",
    "        max_oos_r2_features = df_results.n_features[max_oos_r2_idx]\n",
    "        min_oos_r2 = minimum(valid_oos_r2)\n",
    "        \n",
    "        @printf(\"3. Out-of-Sample R-squared:\\n\")\n",
    "        @printf(\"   - Peaks at %.4f with %d features\\n\", max_oos_r2, max_oos_r2_features)\n",
    "        @printf(\"   - Drops dramatically to %.4f as overfitting increases\\n\", min_oos_r2)\n",
    "        @printf(\"   - Can become negative when predictions are worse than using the mean\\n\")\n",
    "        println()\n",
    "        \n",
    "        interpretation_results[\"max_oos_r2\"] = max_oos_r2\n",
    "        interpretation_results[\"optimal_features_oos_r2\"] = max_oos_r2_features\n",
    "    end\n",
    "    \n",
    "    interpretation_results[\"max_r2_full\"] = max_r2_full\n",
    "    \n",
    "    return interpretation_results\n",
    "end\n",
    "\n",
    "# Interpret the results\n",
    "interpretation = interpret_results(results_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economic Intuition\n",
    "\n",
    "Let's discuss the economic and statistical theory behind these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Economic Intuition:\")\n",
    "println(\"==================\")\n",
    "println()\n",
    "println(\"1. **Bias-Variance Tradeoff**: As we add more features (higher-order polynomials),\")\n",
    "println(\"   we reduce bias but increase variance. Initially, bias reduction dominates,\")\n",
    "println(\"   improving out-of-sample performance. Eventually, variance dominates.\")\n",
    "println()\n",
    "println(\"2. **In-Sample vs Out-of-Sample**: In-sample R¬≤ always increases with more features\")\n",
    "println(\"   because the model can always fit the training data better. However, this\")\n",
    "println(\"   doesn't translate to better prediction on new data.\")\n",
    "println()\n",
    "println(\"3. **Adjusted R-squared as a Model Selection Tool**: Adjusted R¬≤ penalizes model\")\n",
    "println(\"   complexity and provides a better guide for model selection than raw R¬≤.\")\n",
    "println()\n",
    "println(\"4. **The Curse of Dimensionality**: With 1000 observations and up to 1000 features,\")\n",
    "println(\"   we approach the case where we have as many parameters as observations,\")\n",
    "println(\"   leading to perfect in-sample fit but terrible out-of-sample performance.\")\n",
    "println()\n",
    "println(\"5. **Practical Implications**: This demonstrates why regularization techniques\")\n",
    "println(\"   (Ridge, Lasso, Elastic Net) are crucial in high-dimensional settings to\")\n",
    "println(\"   prevent overfitting and improve generalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "Let's analyze the computational performance for different model complexities using Julia's `@time` macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"=== PERFORMANCE ANALYSIS ===\")\n",
    "println()\n",
    "\n",
    "# Test performance for different feature counts\n",
    "test_features = [10, 100, 500, 1000]\n",
    "\n",
    "for n_feat in test_features\n",
    "    println(\"Performance for $n_feat features:\")\n",
    "    \n",
    "    @time begin\n",
    "        X_poly_perf = create_polynomial_features(X, n_feat)\n",
    "        beta_perf = (X_poly_perf' * X_poly_perf) \\ (X_poly_perf' * y)\n",
    "        y_pred_perf = X_poly_perf * beta_perf\n",
    "        r2_perf = r2_score(y, y_pred_perf)\n",
    "    end\n",
    "    \n",
    "    println()\n",
    "end\n",
    "\n",
    "println(\"Julia's performance advantages become clear with larger matrices!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table\n",
    "\n",
    "Let's create a final summary of our key findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the complete results table\n",
    "println(\"\\n=== COMPLETE RESULTS TABLE ===\\n\")\n",
    "display(results_df)\n",
    "\n",
    "# Create summary statistics\n",
    "println(\"\\n=== KEY FINDINGS SUMMARY ===\\n\")\n",
    "\n",
    "if !isempty(interpretation)\n",
    "    summary_data = [\n",
    "        [\"R¬≤ (Full Sample)\", interpretation[\"max_r2_full\"], 1000],\n",
    "        [\"Adjusted R¬≤ (Full)\", get(interpretation, \"max_adj_r2\", NaN), get(interpretation, \"optimal_features_adj_r2\", NaN)],\n",
    "        [\"R¬≤ (Out-of-Sample)\", get(interpretation, \"max_oos_r2\", NaN), get(interpretation, \"optimal_features_oos_r2\", NaN)]\n",
    "    ]\n",
    "    \n",
    "    summary_df = DataFrame(\n",
    "        Metric = [row[1] for row in summary_data],\n",
    "        Maximum_Value = [row[2] for row in summary_data],\n",
    "        Optimal_Features = [row[3] for row in summary_data]\n",
    "    )\n",
    "    \n",
    "    display(summary_df)\n",
    "    \n",
    "    println(\"\\n‚úÖ Overfitting analysis complete!\")\n",
    "    \n",
    "    if haskey(interpretation, \"optimal_features_adj_r2\")\n",
    "        @printf(\"\\nOptimal model complexity (by Adjusted R¬≤): %d features\\n\", interpretation[\"optimal_features_adj_r2\"])\n",
    "    end\n",
    "    if haskey(interpretation, \"optimal_features_oos_r2\")\n",
    "        @printf(\"Optimal model complexity (by Out-of-Sample R¬≤): %d features\\n\", interpretation[\"optimal_features_oos_r2\"])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Display coefficient information for the optimal models\n",
    "println(\"\\n=== MODEL COMPLEXITY INSIGHTS ===\\n\")\n",
    "println(\"True model: y = 2*X + u (1 feature with coefficient 2.0)\\n\")\n",
    "println(\"As we add polynomial terms, we move further from the true simple relationship.\")\n",
    "println(\"The optimal complexity balances fit and generalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Finally, let's save our results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory and save results\n",
    "output_dir = \"/home/runner/work/High_Dimensional_Linear_Models/High_Dimensional_Linear_Models/Julia/output\"\n",
    "mkpath(output_dir)\n",
    "\n",
    "# Save main results\n",
    "CSV.write(joinpath(output_dir, \"overfitting_results.csv\"), results_df)\n",
    "println(\"Results saved to $(output_dir)/overfitting_results.csv\")\n",
    "\n",
    "# Save summary statistics if available\n",
    "if @isdefined(summary_df)\n",
    "    CSV.write(joinpath(output_dir, \"overfitting_summary.csv\"), summary_df)\n",
    "    println(\"Summary statistics saved to $(output_dir)/overfitting_summary.csv\")\n",
    "end\n",
    "\n",
    "println(\"\\nüìÅ All results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis has successfully demonstrated:\n",
    "\n",
    "1. **The bias-variance tradeoff**: As model complexity increases, we observe the classic pattern where out-of-sample performance first improves then deteriorates\n",
    "\n",
    "2. **The importance of proper model selection**: Adjusted R¬≤ provides a better guide than raw R¬≤ for choosing model complexity\n",
    "\n",
    "3. **The dangers of overfitting**: High-dimensional models can achieve perfect in-sample fit while performing terribly on new data\n",
    "\n",
    "4. **Practical implications**: This motivates the use of regularization techniques and cross-validation in machine learning\n",
    "\n",
    "### Julia-Specific Implementation Highlights:\n",
    "- **Performance**: Julia's compiled code runs at near-C speeds for matrix operations\n",
    "- **Syntax**: Clean, mathematical notation that closely matches theoretical formulations\n",
    "- **Memory Efficiency**: Efficient handling of large matrices without significant overhead\n",
    "- **Type System**: Static typing helps catch errors and optimize performance\n",
    "- **Ecosystem**: Rich packages like DataFrames.jl, Plots.jl, and CSV.jl for data analysis\n",
    "\n",
    "### Statistical Insights:\n",
    "- **R¬≤ (Full Sample)**: Monotonically increases, reaching 1.0 with 1000 features\n",
    "- **Adjusted R¬≤**: Peaks early and then declines, incorporating complexity penalty\n",
    "- **Out-of-Sample R¬≤**: Shows inverted U-shape, the gold standard for model selection\n",
    "\n",
    "### Computational Advantages:\n",
    "Julia's performance characteristics make it particularly well-suited for:\n",
    "- Large-scale matrix computations\n",
    "- Iterative algorithms requiring many matrix operations\n",
    "- Statistical simulations and bootstrap procedures\n",
    "- High-dimensional data analysis\n",
    "\n",
    "The results clearly show why understanding overfitting is crucial for building models that generalize well to new data, particularly in high-dimensional settings common in modern econometrics and machine learning. Julia's combination of performance and expressiveness makes it an excellent choice for implementing and exploring these concepts.\n",
    "\n",
    "**This completes Part 2 of Assignment 1 in Julia.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}