\documentclass[11pt,letterpaper]{article}

% Enhanced package imports for professional appearance
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{hyperref}

% Page setup
\geometry{
    margin=1in,
    headheight=14pt,
    footskip=30pt
}

% Color scheme - Elegant Dark Blue and Dark Red Palette
\definecolor{primaryblue}{RGB}{20, 33, 61}         % Deep navy blue
\definecolor{accentblue}{RGB}{31, 81, 128}         % Rich dark blue
\definecolor{darkgray}{RGB}{139, 0, 0}             % Dark elegant red
\definecolor{lightgray}{RGB}{105, 105, 105}        % Dim gray

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\textcolor{darkgray}{High Dimensional Linear Models}}
\fancyhead[R]{\small\textcolor{darkgray}{FWL Theorem Proof}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}

% Section formatting
\titleformat{\section}
{\Large\bfseries\color{primaryblue}}
{\thesection}{1em}{}

\titleformat{\subsection}
{\large\bfseries\color{accentblue}}
{\thesubsection}{1em}{}

\titleformat{\subsubsection}
{\normalsize\bfseries\color{darkgray}}
{\thesubsubsection}{1em}{}

% Theorem environments with clean styling
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

% Custom commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\rank}{\text{rank}}
\newcommand{\tr}{\text{tr}}

% Enhanced theorem styling
\makeatletter
\def\th@definition{%
  \thm@notefont{}%
  \normalfont
  \thm@headfont{\bfseries\color{primaryblue}}%
  \thm@headpunct{.}%
  \thm@preskip\topsep \divide\thm@preskip\tw@
  \thm@postskip\thm@preskip
}
\makeatother

% Proof environment customization
\makeatletter
\renewenvironment{proof}[1][\proofname]{\par
  \pushQED{\qed}%
  \normalfont \topsep6\p@\@plus6\p@\relax
  \trivlist
  \item[\hskip\labelsep
        \itshape\bfseries\color{accentblue}
    #1\@addpunct{.}]\ignorespaces
}{%
  \popQED\endtrivlist\@endpefalse
}
\makeatother

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=primaryblue,
    citecolor=primaryblue,
    urlcolor=primaryblue,
    pdftitle={Frisch-Waugh-Lovell Theorem Proof},
    pdfauthor={gsaco},
    pdfsubject={High Dimensional Linear Models}
}

\title{%
    \vspace{-1cm}
    {\Huge\bfseries\color{primaryblue} Mathematical Proof} \\[0.4cm]
    {\Large\color{accentblue} The Frisch-Waugh-Lovell Theorem} \\[0.8cm]
}

\author{}

\date{}

\begin{document}

\maketitle
\thispagestyle{empty}


\section{Mathematical Foundation}

\subsection{Problem Setup}

Consider the linear regression model:
\begin{equation}\label{eq:model}
\textcolor{primaryblue}{\boxed{y = X_1\beta_1 + X_2\beta_2 + u}}
\end{equation}

\noindent where the components are defined as follows:

\begin{itemize}[itemsep=0.8em, leftmargin=2.5em]
    \item $y \in \R^{n \times 1}$ is the vector of outcomes
    \item $X_1 \in \R^{n \times k_1}$ is the matrix of regressors of interest with $\rank(X_1) = k_1$
    \item $X_2 \in \R^{n \times k_2}$ is the matrix of control variables with $\rank(X_2) = k_2$
    \item $\beta_1 \in \R^{k_1 \times 1}$ is the parameter vector of primary interest
    \item $\beta_2 \in \R^{k_2 \times 1}$ is the parameter vector for control variables
    \item $u \in \R^{n \times 1}$ is the error vector with $\E[u|X_1,X_2] = 0$
\end{itemize}

\begin{definition}[Projection and Annihilator Matrices]\label{def:matrices}
For a full-rank matrix $X_2 \in \R^{n \times k_2}$, we define:

\begin{align}
P_{X_2} &= X_2(X_2'X_2)^{-1}X_2' \quad \text{\textcolor{lightgray}{(Projection matrix)}} \label{eq:projection} \\
M_{X_2} &= I_n - P_{X_2} = I_n - X_2(X_2'X_2)^{-1}X_2' \quad \text{\textcolor{lightgray}{(Annihilator matrix)}} \label{eq:annihilator}
\end{align}

\noindent These matrices satisfy the following fundamental properties:
\begin{enumerate}[label=\textcolor{accentblue}{(\roman*)}, itemsep=0.4em]
    \item $P_{X_2}$ and $M_{X_2}$ are symmetric: $P_{X_2}' = P_{X_2}$, $M_{X_2}' = M_{X_2}$
    \item $P_{X_2}$ and $M_{X_2}$ are idempotent: $P_{X_2}^2 = P_{X_2}$, $M_{X_2}^2 = M_{X_2}$
    \item $M_{X_2}X_2 = 0$ and $P_{X_2}X_2 = X_2$
    \item $P_{X_2} + M_{X_2} = I_n$
\end{enumerate}
\end{definition}

\section{Main Result}

\subsection{The Frisch-Waugh-Lovell Theorem}

\begin{theorem}[Frisch-Waugh-Lovell Theorem]\label{thm:fwl}
The OLS estimate of $\beta_1$ in the full regression of $y$ on $[X_1 \quad X_2]$ is identical to the OLS estimate obtained from the following two-step partialling-out procedure:

\begin{enumerate}[label=\textcolor{primaryblue}{\textbf{Step \arabic*:}}, itemsep=0.6em, leftmargin=3em]
    \item Regress $y$ on $X_2$ and obtain residuals: $\tilde{y} = M_{X_2}y$
    \item Regress $X_1$ on $X_2$ and obtain residuals: $\tilde{X_1} = M_{X_2}X_1$
    \item Regress $\tilde{y}$ on $\tilde{X_1}$ to obtain: $\hat{\beta_1}^{\text{FWL}} = (\tilde{X_1}'\tilde{X_1})^{-1}\tilde{X_1}'\tilde{y}$
\end{enumerate}

\vspace{0.5cm}
\noindent\textbf{\textcolor{primaryblue}{Formal Statement:}} 
\begin{equation}
\textcolor{primaryblue}{\boxed{\hat{\beta_1} = \hat{\beta_1}^{\text{FWL}} = (\tilde{X_1}'\tilde{X_1})^{-1}\tilde{X_1}'\tilde{y}}}
\end{equation}
\end{theorem}

\subsection{Proof}

\begin{proof}
We establish the equivalence by demonstrating that both approaches yield identical coefficient estimates through rigorous matrix algebra.

\subsubsection*{\textcolor{primaryblue}{Part I: Full Regression Setup}}

The full regression model in partitioned form is:
\begin{equation}\label{eq:full_reg}
y = [X_1 \quad X_2]\begin{bmatrix} \beta_1 \\ \beta_2 \end{bmatrix} + u = X\beta + u
\end{equation}

where $X = [X_1 \quad X_2] \in \R^{n \times (k_1+k_2)}$ and $\beta = [\beta_1' \quad \beta_2']' \in \R^{(k_1+k_2) \times 1}$.

The OLS estimator is given by:
\begin{equation}\label{eq:ols}
\hat{\beta} = (X'X)^{-1}X'y = \begin{bmatrix} \hat{\beta_1} \\ \hat{\beta_2} \end{bmatrix}
\end{equation}

\subsubsection*{\textcolor{primaryblue}{Part II: Matrix Partitioning}}

We partition the cross-product matrices:
\begin{align}
X'X &= \begin{bmatrix} X_1'X_1 & X_1'X_2 \\ X_2'X_1 & X_2'X_2 \end{bmatrix} = \begin{bmatrix} A & B \\ B' & D \end{bmatrix} \label{eq:XtX}\\
X'y &= \begin{bmatrix} X_1'y \\ X_2'y \end{bmatrix} \label{eq:Xty}
\end{align}

where:
\begin{align}
A &= X_1'X_1 \in \R^{k_1 \times k_1}, \quad B = X_1'X_2 \in \R^{k_1 \times k_2}, \quad D = X_2'X_2 \in \R^{k_2 \times k_2}
\end{align}

\subsubsection*{\textcolor{primaryblue}{Part III: Partitioned Matrix Inverse}}

Using the block matrix inversion formula:
\begin{equation}\label{eq:partitioned_inverse}
(X'X)^{-1} = \begin{bmatrix} 
(A - BD^{-1}B')^{-1} & -(A - BD^{-1}B')^{-1}BD^{-1} \\
-D^{-1}B'(A - BD^{-1}B')^{-1} & D^{-1} + D^{-1}B'(A - BD^{-1}B')^{-1}BD^{-1}
\end{bmatrix}
\end{equation}

\subsubsection*{\textcolor{primaryblue}{Part IV: Key Algebraic Identity}}

We establish the fundamental relationship:
\begin{align}
A - BD^{-1}B' &= X_1'X_1 - X_1'X_2(X_2'X_2)^{-1}X_2'X_1 \\
&= X_1'(I_n - X_2(X_2'X_2)^{-1}X_2')X_1 \\
&= X_1'M_{X_2}X_1 \label{eq:key_identity}
\end{align}

\subsubsection*{\textcolor{primaryblue}{Part V: Extracting $\hat{\beta_1}$}}

From the normal equations $(X'X)\hat{\beta} = X'y$, the first block gives us:
\begin{align}
\hat{\beta_1} &= (A - BD^{-1}B')^{-1}(X_1'y - BD^{-1}X_2'y) \\
&= (X_1'M_{X_2}X_1)^{-1}(X_1'y - X_1'X_2(X_2'X_2)^{-1}X_2'y) \\
&= (X_1'M_{X_2}X_1)^{-1}X_1'(I_n - X_2(X_2'X_2)^{-1}X_2')y \\
&= (X_1'M_{X_2}X_1)^{-1}X_1'M_{X_2}y \label{eq:beta1_full}
\end{align}

\subsubsection*{\textcolor{primaryblue}{Part VI: Two-Step Procedure Analysis}}

The partialling-out procedure yields:
\begin{align}
\tilde{y} &= M_{X_2}y \quad \text{\textcolor{lightgray}{(Step 1)}} \\
\tilde{X_1} &= M_{X_2}X_1 \quad \text{\textcolor{lightgray}{(Step 2)}} \\
\hat{\beta_1}^{\text{FWL}} &= (\tilde{X_1}'\tilde{X_1})^{-1}\tilde{X_1}'\tilde{y} \quad \text{\textcolor{lightgray}{(Step 3)}} \label{eq:beta1_fwl}
\end{align}

\subsubsection*{\textcolor{primaryblue}{Part VII: Establishing Equivalence}}

Substituting the definitions from Steps 1 and 2:
\begin{align}
\hat{\beta_1}^{\text{FWL}} &= ((M_{X_2}X_1)'(M_{X_2}X_1))^{-1}(M_{X_2}X_1)'(M_{X_2}y) \\
&= (X_1'M_{X_2}'M_{X_2}X_1)^{-1}X_1'M_{X_2}'M_{X_2}y \label{eq:substitution}
\end{align}

\subsubsection*{\textcolor{primaryblue}{Part VIII: Applying Matrix Properties}}

Using the symmetry and idempotency of $M_{X_2}$ from Definition \ref{def:matrices}:
\begin{align}
M_{X_2}' &= M_{X_2} \quad \text{\textcolor{lightgray}{(symmetry)}} \\
M_{X_2}M_{X_2} &= M_{X_2} \quad \text{\textcolor{lightgray}{(idempotency)}}
\end{align}

Therefore:
\begin{align}
\hat{\beta_1}^{\text{FWL}} &= (X_1'M_{X_2}M_{X_2}X_1)^{-1}X_1'M_{X_2}M_{X_2}y \\
&= (X_1'M_{X_2}X_1)^{-1}X_1'M_{X_2}y \label{eq:final_fwl}
\end{align}

\subsubsection*{\textcolor{primaryblue}{Part IX: Final Equivalence}}

Comparing equations \eqref{eq:beta1_full} and \eqref{eq:final_fwl}:
\begin{equation}
\hat{\beta_1} = (X_1'M_{X_2}X_1)^{-1}X_1'M_{X_2}y = \hat{\beta_1}^{\text{FWL}}
\end{equation}

This establishes the desired result:
\begin{equation}
\textcolor{primaryblue}{\boxed{\hat{\beta_1} = (\tilde{X_1}'\tilde{X_1})^{-1}\tilde{X_1}'\tilde{y}}}
\end{equation}
\end{proof}

\end{document}