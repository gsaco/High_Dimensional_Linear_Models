{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Part 2: Overfitting Analysis (CORRECTED)\n",
    "## Overfitting (8 points)\n",
    "\n",
    "This notebook analyzes overfitting using the correct data generating process from the class example:\n",
    "**y = exp(4*W) + e**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Following the class example: **y = exp(4*W) + e**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate data following the class example specification:\n",
    "    y = np.exp(4 * W) + e\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n : int\n",
    "        Sample size (default: 1000)\n",
    "    seed : int\n",
    "        Random seed for reproducibility (42)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    W : numpy.ndarray\n",
    "        Feature matrix (n x 1) - sorted uniform random variables\n",
    "    y : numpy.ndarray\n",
    "        Target variable (n,) following y = exp(4*W) + e\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate W from uniform distribution and sort (as in class example)\n",
    "    W = np.random.uniform(0, 1, n)\n",
    "    W.sort()\n",
    "    W = W.reshape(-1, 1)\n",
    "    \n",
    "    # Generate error term\n",
    "    e = np.random.normal(0, 1, n)\n",
    "    \n",
    "    # Generate y following class example: y = exp(4*W) + e\n",
    "    y = np.exp(4 * W.ravel()) + e\n",
    "    \n",
    "    return W, y\n",
    "\n",
    "# Generate the data\n",
    "W, y = generate_data(n=1000, seed=42)\n",
    "\n",
    "print(f\"Generated data with n={len(y)} observations\")\n",
    "print(f\"True relationship: y = exp(4*W) + e\")\n",
    "print(f\"W range: [{W.min():.4f}, {W.max():.4f}]\")\n",
    "print(f\"y range: [{y.min():.4f}, {y.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_features(W, n_features):\n",
    "    \"\"\"\n",
    "    Create polynomial features up to n_features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    W : numpy.ndarray\n",
    "        Original feature matrix (n x 1)\n",
    "    n_features : int\n",
    "        Number of features to create\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    W_poly : numpy.ndarray\n",
    "        Extended feature matrix with polynomial features\n",
    "    \"\"\"\n",
    "    n_samples = W.shape[0]\n",
    "    W_poly = np.zeros((n_samples, n_features))\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        W_poly[:, i] = W.ravel() ** (i + 1)  # W^1, W^2, W^3, etc.\n",
    "    \n",
    "    return W_poly\n",
    "\n",
    "def calculate_adjusted_r2(r2, n, k):\n",
    "    \"\"\"\n",
    "    Calculate adjusted R-squared.\n",
    "    \n",
    "    Adjusted R² = 1 - [(1 - R²)(n - 1) / (n - k - 1)]\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    r2 : float\n",
    "        R-squared value\n",
    "    n : int\n",
    "        Sample size\n",
    "    k : int\n",
    "        Number of features (excluding intercept)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    adj_r2 : float\n",
    "        Adjusted R-squared\n",
    "    \"\"\"\n",
    "    # Handle edge cases where we have too many features\n",
    "    if n - k - 1 <= 0:\n",
    "        return np.nan\n",
    "    \n",
    "    adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "    return adj_r2\n",
    "\n",
    "# Test the functions\n",
    "W_poly_example = create_polynomial_features(W, 5)\n",
    "print(f\"Original W shape: {W.shape}\")\n",
    "print(f\"Polynomial features (5 features) shape: {W_poly_example.shape}\")\n",
    "print(f\"Example adjusted R²: {calculate_adjusted_r2(0.8, 1000, 5):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Analysis\n",
    "\n",
    "Test models with different numbers of polynomial features: 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfitting_analysis():\n",
    "    \"\"\"\n",
    "    Main function to perform overfitting analysis.\n",
    "    \"\"\"\n",
    "    # Number of features to test (as specified)\n",
    "    n_features_list = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "    \n",
    "    # Storage for results\n",
    "    results = []\n",
    "    \n",
    "    print(\"Analyzing overfitting for different numbers of features...\")\n",
    "    print(\"Features | R² (full) | Adj R² (full) | R² (out-of-sample)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for n_feat in n_features_list:\n",
    "        try:\n",
    "            # Create polynomial features\n",
    "            W_poly = create_polynomial_features(W, n_feat)\n",
    "            \n",
    "            # Split data into train/test (75%/25%)\n",
    "            W_train, W_test, y_train, y_test = train_test_split(\n",
    "                W_poly, y, test_size=0.25, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Fit model on full sample (with intercept for proper estimation)\n",
    "            model_full = LinearRegression(fit_intercept=True)\n",
    "            model_full.fit(W_poly, y)\n",
    "            y_pred_full = model_full.predict(W_poly)\n",
    "            r2_full = r2_score(y, y_pred_full)\n",
    "            \n",
    "            # Calculate adjusted R²\n",
    "            adj_r2_full = calculate_adjusted_r2(r2_full, len(y), n_feat)\n",
    "            \n",
    "            # Fit model on training data and predict on test data\n",
    "            model_train = LinearRegression(fit_intercept=True)\n",
    "            model_train.fit(W_train, y_train)\n",
    "            y_pred_test = model_train.predict(W_test)\n",
    "            r2_out_of_sample = r2_score(y_test, y_pred_test)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'n_features': n_feat,\n",
    "                'r2_full': r2_full,\n",
    "                'adj_r2_full': adj_r2_full,\n",
    "                'r2_out_of_sample': r2_out_of_sample\n",
    "            })\n",
    "            \n",
    "            print(f\"{n_feat:8d} | {r2_full:9.4f} | {adj_r2_full:12.4f} | {r2_out_of_sample:17.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {n_feat} features: {str(e)}\")\n",
    "            # Still append to maintain consistency\n",
    "            results.append({\n",
    "                'n_features': n_feat,\n",
    "                'r2_full': np.nan,\n",
    "                'adj_r2_full': np.nan,\n",
    "                'r2_out_of_sample': np.nan\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the analysis\n",
    "results_df = overfitting_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Create three separate graphs for each R-squared measure as requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_separate_plots(df_results):\n",
    "    \"\"\"\n",
    "    Create three separate plots for R-squared analysis.\n",
    "    \"\"\"\n",
    "    # Filter out NaN values for plotting\n",
    "    df_clean = df_results.dropna()\n",
    "    \n",
    "    if df_clean.empty:\n",
    "        print(\"No valid results to plot\")\n",
    "        return None\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot 1: R-squared (full sample)\n",
    "    axes[0].plot(df_clean['n_features'], df_clean['r2_full'], \n",
    "                marker='o', linewidth=2, markersize=6, color='blue')\n",
    "    axes[0].set_title('R-squared on Full Sample vs Number of Features', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Number of Features')\n",
    "    axes[0].set_ylabel('R-squared')\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    # Plot 2: Adjusted R-squared (full sample)\n",
    "    axes[1].plot(df_clean['n_features'], df_clean['adj_r2_full'], \n",
    "                marker='s', linewidth=2, markersize=6, color='green')\n",
    "    axes[1].set_title('Adjusted R-squared on Full Sample vs Number of Features', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Number of Features')\n",
    "    axes[1].set_ylabel('Adjusted R-squared')\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Out-of-sample R-squared\n",
    "    axes[2].plot(df_clean['n_features'], df_clean['r2_out_of_sample'], \n",
    "                marker='^', linewidth=2, markersize=6, color='red')\n",
    "    axes[2].set_title('Out-of-Sample R-squared vs Number of Features', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_xlabel('Number of Features')\n",
    "    axes[2].set_ylabel('Out-of-Sample R-squared')\n",
    "    axes[2].set_xscale('log')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create the plots\n",
    "fig = create_separate_plots(results_df)\n",
    "\n",
    "print(\"\\nThree separate plots created showing:\")\n",
    "print(\"1. R² (Full Sample): Should show monotonic increase\")\n",
    "print(\"2. Adjusted R² (Full Sample): Should show peak and decline due to complexity penalty\")\n",
    "print(\"3. R² (Out-of-Sample): Should show the classic overfitting pattern (inverted U-shape)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display complete results\n",
    "print(\"\\n=== COMPLETE RESULTS TABLE ===\")\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Find optimal complexity\n",
    "valid_results = results_df.dropna()\n",
    "if not valid_results.empty:\n",
    "    optimal_adj_r2_idx = valid_results['adj_r2_full'].idxmax()\n",
    "    optimal_oos_r2_idx = valid_results['r2_out_of_sample'].idxmax()\n",
    "    \n",
    "    print(\"\\n=== OPTIMAL MODEL COMPLEXITY ===\")\n",
    "    print(f\"By Adjusted R²: {valid_results.loc[optimal_adj_r2_idx, 'n_features']} features\")\n",
    "    print(f\"  - Adjusted R² = {valid_results.loc[optimal_adj_r2_idx, 'adj_r2_full']:.4f}\")\n",
    "    print(f\"By Out-of-Sample R²: {valid_results.loc[optimal_oos_r2_idx, 'n_features']} features\")\n",
    "    print(f\"  - Out-of-Sample R² = {valid_results.loc[optimal_oos_r2_idx, 'r2_out_of_sample']:.4f}\")\n",
    "\n",
    "print(\"\\n=== INSIGHTS ===\")\n",
    "print(\"✅ This analysis demonstrates the classic bias-variance tradeoff\")\n",
    "print(\"📈 R² (Full Sample) should increase monotonically with model complexity\")\n",
    "print(\"📊 Adjusted R² should peak early and then decline due to complexity penalty\")\n",
    "print(\"📉 Out-of-Sample R² should show the inverted U-shape characteristic of overfitting\")\n",
    "print(\"🎯 True model follows: y = exp(4*W) + e\")\n",
    "print(\"⚠️ High-dimensional models (many features) lead to severe overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '../output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(f'{output_dir}/overfitting_results_corrected.csv', index=False)\n",
    "print(f\"Results saved to {output_dir}/overfitting_results_corrected.csv\")\n",
    "\n",
    "print(\"\\n🎉 CORRECTED overfitting analysis complete!\")\n",
    "print(\"Data generation follows class example with:\")\n",
    "print(\"- W ~ Uniform(0,1), sorted, n=1000\")\n",
    "print(\"- e ~ Normal(0,1)\")\n",
    "print(\"- y = exp(4*W) + e (class example)\")\n",
    "print(\"- With intercept for proper estimation\")\n",
    "print(\"- Seed = 42 for reproducibility\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}