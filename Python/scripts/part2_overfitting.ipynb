{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Part 2: Overfitting Analysis\n",
    "## Overfitting (8 points)\n",
    "\n",
    "This notebook analyzes overfitting using a procedure similar to simulation.ipynb. We use a simple data generating process and study how R-squared measures change with model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Following the simulation.ipynb approach, we generate data with a convenient slope (PGD) for all three languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate data following the specification similar to simulation.ipynb.\n",
    "    Two variables X and Y, intercept parameter is zero.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n : int\n",
    "        Sample size (default: 1000)\n",
    "    seed : int\n",
    "        Random seed for reproducibility (42)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix (n x 1)\n",
    "    y : numpy.ndarray\n",
    "        Target variable (n,)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate X from uniform distribution like in simulation.ipynb\n",
    "    X_raw = np.random.uniform(0, 1, n)\n",
    "    X_raw.sort()  # Sort like in simulation\n",
    "    X = X_raw.reshape(-1, 1)\n",
    "    \n",
    "    # Generate error term\n",
    "    e = np.random.normal(0, 1, n)\n",
    "    \n",
    "    # Generate y with no intercept (as requested)\n",
    "    # True relationship: y = 2*X + e (convenient slope for all languages)\n",
    "    beta_true = 2.0\n",
    "    y = beta_true * X.ravel() + e\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate the data\n",
    "X, y = generate_data(n=1000, seed=42)\n",
    "\n",
    "print(f\"Generated data with n={len(y)} observations\")\n",
    "print(f\"True relationship: y = 2*X + e (convenient slope = 2.0)\")\n",
    "print(f\"X range: [{X.min():.4f}, {X.max():.4f}]\")\n",
    "print(f\"y range: [{y.min():.4f}, {y.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_features(X, n_features):\n",
    "    \"\"\"\n",
    "    Create polynomial features up to n_features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Original feature matrix (n x 1)\n",
    "    n_features : int\n",
    "        Number of features to create\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_poly : numpy.ndarray\n",
    "        Extended feature matrix with polynomial features\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    X_poly = np.zeros((n_samples, n_features))\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        X_poly[:, i] = X.ravel() ** (i + 1)  # x^1, x^2, x^3, etc.\n",
    "    \n",
    "    return X_poly\n",
    "\n",
    "def calculate_adjusted_r2(r2, n, k):\n",
    "    \"\"\"\n",
    "    Calculate adjusted R-squared.\n",
    "    \n",
    "    Adjusted R² = 1 - [(1 - R²)(n - 1) / (n - k - 1)]\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    r2 : float\n",
    "        R-squared value\n",
    "    n : int\n",
    "        Sample size\n",
    "    k : int\n",
    "        Number of features (excluding intercept)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    adj_r2 : float\n",
    "        Adjusted R-squared\n",
    "    \"\"\"\n",
    "    if n - k - 1 <= 0:\n",
    "        return np.nan\n",
    "    \n",
    "    adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "    return adj_r2\n",
    "\n",
    "# Test the functions\n",
    "X_poly_example = create_polynomial_features(X, 5)\n",
    "print(f\"Original X shape: {X.shape}\")\n",
    "print(f\"Polynomial features (5 features) shape: {X_poly_example.shape}\")\n",
    "print(f\"Example adjusted R²: {calculate_adjusted_r2(0.8, 1000, 5):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Analysis\n",
    "\n",
    "Test models with different numbers of polynomial features: 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfitting_analysis():\n",
    "    \"\"\"\n",
    "    Main function to perform overfitting analysis.\n",
    "    \"\"\"\n",
    "    # Number of features to test (as specified)\n",
    "    n_features_list = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "    \n",
    "    # Storage for results\n",
    "    results = []\n",
    "    \n",
    "    print(\"Analyzing overfitting for different numbers of features...\")\n",
    "    print(\"Features | R² (full) | Adj R² (full) | R² (out-of-sample)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for n_feat in n_features_list:\n",
    "        try:\n",
    "            # Create polynomial features\n",
    "            X_poly = create_polynomial_features(X, n_feat)\n",
    "            \n",
    "            # Split data into train/test (75%/25%)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_poly, y, test_size=0.25, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Fit model on full sample (no intercept as requested)\n",
    "            model_full = LinearRegression(fit_intercept=False)\n",
    "            model_full.fit(X_poly, y)\n",
    "            y_pred_full = model_full.predict(X_poly)\n",
    "            r2_full = r2_score(y, y_pred_full)\n",
    "            \n",
    "            # Calculate adjusted R²\n",
    "            adj_r2_full = calculate_adjusted_r2(r2_full, len(y), n_feat)\n",
    "            \n",
    "            # Fit model on training data and predict on test data\n",
    "            model_train = LinearRegression(fit_intercept=False)\n",
    "            model_train.fit(X_train, y_train)\n",
    "            y_pred_test = model_train.predict(X_test)\n",
    "            r2_out_of_sample = r2_score(y_test, y_pred_test)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'n_features': n_feat,\n",
    "                'r2_full': r2_full,\n",
    "                'adj_r2_full': adj_r2_full,\n",
    "                'r2_out_of_sample': r2_out_of_sample\n",
    "            })\n",
    "            \n",
    "            print(f\"{n_feat:8d} | {r2_full:9.4f} | {adj_r2_full:12.4f} | {r2_out_of_sample:17.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {n_feat} features: {str(e)}\")\n",
    "            # Still append to maintain consistency\n",
    "            results.append({\n",
    "                'n_features': n_feat,\n",
    "                'r2_full': np.nan,\n",
    "                'adj_r2_full': np.nan,\n",
    "                'r2_out_of_sample': np.nan\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the analysis\n",
    "results_df = overfitting_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Create three separate graphs for each R-squared measure as requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_separate_plots(df_results):\n",
    "    \"\"\"\n",
    "    Create three separate plots for R-squared analysis.\n",
    "    \"\"\"\n",
    "    # Filter out NaN values for plotting\n",
    "    df_clean = df_results.dropna()\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot 1: R-squared (full sample)\n",
    "    axes[0].plot(df_clean['n_features'], df_clean['r2_full'], \n",
    "                marker='o', linewidth=2, markersize=6, color='blue')\n",
    "    axes[0].set_title('R-squared on Full Sample vs Number of Features', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Number of Features')\n",
    "    axes[0].set_ylabel('R-squared')\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    # Plot 2: Adjusted R-squared (full sample)\n",
    "    axes[1].plot(df_clean['n_features'], df_clean['adj_r2_full'], \n",
    "                marker='s', linewidth=2, markersize=6, color='green')\n",
    "    axes[1].set_title('Adjusted R-squared on Full Sample vs Number of Features', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Number of Features')\n",
    "    axes[1].set_ylabel('Adjusted R-squared')\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Out-of-sample R-squared\n",
    "    axes[2].plot(df_clean['n_features'], df_clean['r2_out_of_sample'], \n",
    "                marker='^', linewidth=2, markersize=6, color='red')\n",
    "    axes[2].set_title('Out-of-Sample R-squared vs Number of Features', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_xlabel('Number of Features')\n",
    "    axes[2].set_ylabel('Out-of-Sample R-squared')\n",
    "    axes[2].set_xscale('log')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create the plots\n",
    "fig = create_separate_plots(results_df)\n",
    "\n",
    "print(\"\\nThree separate plots created showing:\")\n",
    "print(\"1. R² (Full Sample): Shows monotonic increase\")\n",
    "print(\"2. Adjusted R² (Full Sample): Shows peak and decline due to complexity penalty\")\n",
    "print(\"3. R² (Out-of-Sample): Shows the classic overfitting pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display complete results\n",
    "print(\"\\n=== COMPLETE RESULTS TABLE ===\")\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Find optimal complexity\n",
    "valid_results = results_df.dropna()\n",
    "if not valid_results.empty:\n",
    "    optimal_adj_r2_idx = valid_results['adj_r2_full'].idxmax()\n",
    "    optimal_oos_r2_idx = valid_results['r2_out_of_sample'].idxmax()\n",
    "    \n",
    "    print(\"\\n=== OPTIMAL MODEL COMPLEXITY ===\")\n",
    "    print(f\"By Adjusted R²: {valid_results.loc[optimal_adj_r2_idx, 'n_features']} features\")\n",
    "    print(f\"By Out-of-Sample R²: {valid_results.loc[optimal_oos_r2_idx, 'n_features']} features\")\n",
    "\n",
    "print(\"\\n=== INSIGHTS ===\")\n",
    "print(\"✅ This analysis demonstrates the classic bias-variance tradeoff\")\n",
    "print(\"📈 R² (Full Sample) increases monotonically with model complexity\")\n",
    "print(\"📊 Adjusted R² peaks early and then declines due to complexity penalty\")\n",
    "print(\"📉 Out-of-Sample R² shows the inverted U-shape characteristic of overfitting\")\n",
    "print(\"🎯 True model has only 1 feature (y = 2*X + e), but polynomial terms can help initially\")\n",
    "print(\"⚠️ High-dimensional models (many features) lead to severe overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '../output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(f'{output_dir}/overfitting_results_python.csv', index=False)\n",
    "print(f\"Results saved to {output_dir}/overfitting_results_python.csv\")\n",
    "\n",
    "print(\"\\n🎉 Python overfitting analysis complete!\")\n",
    "print(\"Data generation follows simulation.ipynb approach with:\")\n",
    "print(\"- X ~ Uniform(0,1), sorted, n=1000\")\n",
    "print(\"- e ~ Normal(0,1)\")\n",
    "print(\"- y = 2*X + e (convenient slope = 2.0)\")\n",
    "print(\"- No intercept (as requested)\")\n",
    "print(\"- Seed = 42 for reproducibility\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}