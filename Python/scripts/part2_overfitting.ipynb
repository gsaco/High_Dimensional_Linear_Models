{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Part 2: Overfitting Analysis\n",
    "## Overfitting (8 points)\n",
    "\n",
    "This notebook simulates a data generating process and analyzes overfitting by estimating linear models with increasing numbers of polynomial features.\n",
    "\n",
    "We will demonstrate the classic bias-variance tradeoff by examining how different R-squared measures behave as model complexity increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We'll generate data following a simple relationship: y = 2X + u, where u is random noise. This gives us a known ground truth to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate data following the specification with only 2 variables X and Y.\n",
    "    Intercept parameter is set to zero as requested.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n : int\n",
    "        Sample size (default: 1000)\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix\n",
    "    y : numpy.ndarray\n",
    "        Target variable\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate X (single feature initially)\n",
    "    X = np.random.randn(n, 1)\n",
    "    \n",
    "    # Generate error term\n",
    "    u = np.random.randn(n)\n",
    "    \n",
    "    # Generate y with no intercept (as requested)\n",
    "    # True relationship: y = 2*X + u\n",
    "    beta_true = 2.0\n",
    "    y = beta_true * X.ravel() + u\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate the data\n",
    "X, y = generate_data(n=1000, seed=42)\n",
    "\n",
    "print(f\"Generated data with n={len(y)} observations\")\n",
    "print(f\"True relationship: y = 2*X + u\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X mean: {X.mean():.4f}, X std: {X.std():.4f}\")\n",
    "print(f\"y mean: {y.mean():.4f}, y std: {y.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Feature Creation\n",
    "\n",
    "We'll create polynomial features of increasing complexity to study overfitting behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_features(X, n_features):\n",
    "    \"\"\"\n",
    "    Create polynomial features up to n_features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Original feature matrix (n x 1)\n",
    "    n_features : int\n",
    "        Number of features to create\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_poly : numpy.ndarray\n",
    "        Extended feature matrix with polynomial features\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    X_poly = np.zeros((n_samples, n_features))\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        if i == 0:\n",
    "            X_poly[:, i] = X.ravel()  # x^1\n",
    "        else:\n",
    "            X_poly[:, i] = X.ravel() ** (i + 1)  # x^2, x^3, etc.\n",
    "    \n",
    "    return X_poly\n",
    "\n",
    "# Example: create polynomial features\n",
    "X_poly_example = create_polynomial_features(X, 5)\n",
    "print(f\"Example: Original X shape: {X.shape}\")\n",
    "print(f\"Example: Polynomial features (5 features) shape: {X_poly_example.shape}\")\n",
    "print(f\"First 5 rows of polynomial features:\")\n",
    "print(X_poly_example[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted R-squared Calculation\n",
    "\n",
    "We'll implement adjusted R-squared, which penalizes model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adjusted_r2(r2, n, k):\n",
    "    \"\"\"\n",
    "    Calculate adjusted R-squared.\n",
    "    \n",
    "    Adjusted R² = 1 - [(1 - R²)(n - 1) / (n - k - 1)]\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    r2 : float\n",
    "        R-squared value\n",
    "    n : int\n",
    "        Sample size\n",
    "    k : int\n",
    "        Number of features (excluding intercept)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    adj_r2 : float\n",
    "        Adjusted R-squared\n",
    "    \"\"\"\n",
    "    if n - k - 1 <= 0:\n",
    "        return np.nan\n",
    "    \n",
    "    adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "    return adj_r2\n",
    "\n",
    "# Example calculation\n",
    "example_r2 = 0.8\n",
    "example_n = 1000\n",
    "example_k = 5\n",
    "example_adj_r2 = calculate_adjusted_r2(example_r2, example_n, example_k)\n",
    "print(f\"Example: R² = {example_r2}, n = {example_n}, k = {example_k}\")\n",
    "print(f\"Adjusted R² = {example_adj_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Analysis\n",
    "\n",
    "Now we'll perform the main analysis, testing models with different numbers of polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfitting_analysis():\n",
    "    \"\"\"\n",
    "    Main function to perform overfitting analysis.\n",
    "    \"\"\"\n",
    "    print(\"=== OVERFITTING ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Number of features to test\n",
    "    n_features_list = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "    \n",
    "    # Storage for results\n",
    "    results = {\n",
    "        'n_features': [],\n",
    "        'r2_full': [],\n",
    "        'adj_r2_full': [],\n",
    "        'r2_out_of_sample': []\n",
    "    }\n",
    "    \n",
    "    print(\"Analyzing overfitting for different numbers of features...\")\n",
    "    print(\"Features | R² (full) | Adj R² (full) | R² (out-of-sample)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for n_feat in n_features_list:\n",
    "        try:\n",
    "            # Create polynomial features\n",
    "            X_poly = create_polynomial_features(X, n_feat)\n",
    "            \n",
    "            # Split data into train/test (75%/25%)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_poly, y, test_size=0.25, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Fit model on full sample\n",
    "            reg_full = LinearRegression(fit_intercept=False)  # No intercept as requested\n",
    "            reg_full.fit(X_poly, y)\n",
    "            y_pred_full = reg_full.predict(X_poly)\n",
    "            r2_full = r2_score(y, y_pred_full)\n",
    "            \n",
    "            # Calculate adjusted R²\n",
    "            adj_r2_full = calculate_adjusted_r2(r2_full, len(y), n_feat)\n",
    "            \n",
    "            # Fit model on training data and predict on test data\n",
    "            reg_train = LinearRegression(fit_intercept=False)\n",
    "            reg_train.fit(X_train, y_train)\n",
    "            y_pred_test = reg_train.predict(X_test)\n",
    "            r2_out_of_sample = r2_score(y_test, y_pred_test)\n",
    "            \n",
    "            # Store results\n",
    "            results['n_features'].append(n_feat)\n",
    "            results['r2_full'].append(r2_full)\n",
    "            results['adj_r2_full'].append(adj_r2_full)\n",
    "            results['r2_out_of_sample'].append(r2_out_of_sample)\n",
    "            \n",
    "            print(f\"{n_feat:8d} | {r2_full:9.4f} | {adj_r2_full:12.4f} | {r2_out_of_sample:17.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {n_feat} features: {e}\")\n",
    "            # Still append to maintain list length\n",
    "            results['n_features'].append(n_feat)\n",
    "            results['r2_full'].append(np.nan)\n",
    "            results['adj_r2_full'].append(np.nan)\n",
    "            results['r2_out_of_sample'].append(np.nan)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# Run the analysis\n",
    "results_df = overfitting_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's create three plots to visualize the different R-squared measures as a function of model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots(df_results):\n",
    "    \"\"\"\n",
    "    Create three separate plots for R-squared analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_results : pandas.DataFrame\n",
    "        Results from overfitting analysis\n",
    "    \"\"\"\n",
    "    print(\"Creating plots...\")\n",
    "    \n",
    "    # Set up the plotting parameters\n",
    "    fig_size = (12, 5)\n",
    "    \n",
    "    # Plot 1: R-squared (full sample)\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.plot(df_results['n_features'], df_results['r2_full'], \n",
    "             marker='o', linewidth=2, markersize=6, color='blue')\n",
    "    plt.title('R-squared on Full Sample vs Number of Features', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Number of Features', fontsize=12)\n",
    "    plt.ylabel('R-squared', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 2: Adjusted R-squared (full sample)\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.plot(df_results['n_features'], df_results['adj_r2_full'], \n",
    "             marker='s', linewidth=2, markersize=6, color='green')\n",
    "    plt.title('Adjusted R-squared on Full Sample vs Number of Features', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Number of Features', fontsize=12)\n",
    "    plt.ylabel('Adjusted R-squared', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 3: Out-of-sample R-squared\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.plot(df_results['n_features'], df_results['r2_out_of_sample'], \n",
    "             marker='^', linewidth=2, markersize=6, color='red')\n",
    "    plt.title('Out-of-Sample R-squared vs Number of Features', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Number of Features', fontsize=12)\n",
    "    plt.ylabel('Out-of-Sample R-squared', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Plots created successfully!\")\n",
    "\n",
    "# Create the plots\n",
    "create_plots(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Interpretation\n",
    "\n",
    "Let's analyze the patterns we observe and understand the economic intuition behind them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_results(df_results):\n",
    "    \"\"\"\n",
    "    Provide interpretation and intuition for the results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_results : pandas.DataFrame\n",
    "        Results from overfitting analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RESULTS INTERPRETATION ===\\n\")\n",
    "    \n",
    "    print(\"Key Observations:\")\n",
    "    print(\"================\")\n",
    "    \n",
    "    # R-squared observations\n",
    "    max_r2_full = df_results['r2_full'].max()\n",
    "    max_r2_features = df_results.loc[df_results['r2_full'].idxmax(), 'n_features']\n",
    "    \n",
    "    print(f\"1. R-squared (Full Sample):\")\n",
    "    print(f\"   - Starts at {df_results['r2_full'].iloc[0]:.4f} with 1 feature\")\n",
    "    print(f\"   - Reaches maximum of {max_r2_full:.4f} with {max_r2_features} features\")\n",
    "    print(f\"   - Shows monotonic increase as expected in in-sample fit\")\n",
    "    print()\n",
    "    \n",
    "    # Adjusted R-squared observations\n",
    "    max_adj_r2 = df_results['adj_r2_full'].max()\n",
    "    max_adj_r2_features = df_results.loc[df_results['adj_r2_full'].idxmax(), 'n_features']\n",
    "    \n",
    "    print(f\"2. Adjusted R-squared (Full Sample):\")\n",
    "    print(f\"   - Peaks at {max_adj_r2:.4f} with {max_adj_r2_features} features\")\n",
    "    print(f\"   - Then declines as the penalty for additional features outweighs benefit\")\n",
    "    print(f\"   - Becomes negative when model is severely overfitted\")\n",
    "    print()\n",
    "    \n",
    "    # Out-of-sample observations\n",
    "    max_oos_r2 = df_results['r2_out_of_sample'].max()\n",
    "    max_oos_r2_features = df_results.loc[df_results['r2_out_of_sample'].idxmax(), 'n_features']\n",
    "    min_oos_r2 = df_results['r2_out_of_sample'].min()\n",
    "    \n",
    "    print(f\"3. Out-of-Sample R-squared:\")\n",
    "    print(f\"   - Peaks at {max_oos_r2:.4f} with {max_oos_r2_features} features\")\n",
    "    print(f\"   - Drops dramatically to {min_oos_r2:.4f} as overfitting increases\")\n",
    "    print(f\"   - Can become negative when predictions are worse than using the mean\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        'max_r2_full': max_r2_full,\n",
    "        'max_adj_r2': max_adj_r2,\n",
    "        'max_oos_r2': max_oos_r2,\n",
    "        'optimal_features_adj_r2': max_adj_r2_features,\n",
    "        'optimal_features_oos_r2': max_oos_r2_features\n",
    "    }\n",
    "\n",
    "# Interpret the results\n",
    "interpretation = interpret_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economic Intuition\n",
    "\n",
    "Let's discuss the economic and statistical theory behind these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Economic Intuition:\")\n",
    "print(\"==================\")\n",
    "print()\n",
    "print(\"1. **Bias-Variance Tradeoff**: As we add more features (higher-order polynomials),\")\n",
    "print(\"   we reduce bias but increase variance. Initially, bias reduction dominates,\")\n",
    "print(\"   improving out-of-sample performance. Eventually, variance dominates.\")\n",
    "print()\n",
    "print(\"2. **In-Sample vs Out-of-Sample**: In-sample R² always increases with more features\")\n",
    "print(\"   because the model can always fit the training data better. However, this\")\n",
    "print(\"   doesn't translate to better prediction on new data.\")\n",
    "print()\n",
    "print(\"3. **Adjusted R-squared as a Model Selection Tool**: Adjusted R² penalizes model\")\n",
    "print(\"   complexity and provides a better guide for model selection than raw R².\")\n",
    "print()\n",
    "print(\"4. **The Curse of Dimensionality**: With 1000 observations and up to 1000 features,\")\n",
    "print(\"   we approach the case where we have as many parameters as observations,\")\n",
    "print(\"   leading to perfect in-sample fit but terrible out-of-sample performance.\")\n",
    "print()\n",
    "print(\"5. **Practical Implications**: This demonstrates why regularization techniques\")\n",
    "print(\"   (Ridge, Lasso, Elastic Net) are crucial in high-dimensional settings to\")\n",
    "print(\"   prevent overfitting and improve generalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table\n",
    "\n",
    "Let's create a final summary of our key findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results table\n",
    "print(\"\\n=== COMPLETE RESULTS TABLE ===\\n\")\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Create summary statistics\n",
    "print(\"\\n=== KEY FINDINGS SUMMARY ===\\n\")\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Metric': ['R² (Full Sample)', 'Adjusted R² (Full)', 'R² (Out-of-Sample)'],\n",
    "    'Maximum Value': [interpretation['max_r2_full'], \n",
    "                     interpretation['max_adj_r2'], \n",
    "                     interpretation['max_oos_r2']],\n",
    "    'Optimal # Features': [1000,  # R² always increases\n",
    "                          interpretation['optimal_features_adj_r2'],\n",
    "                          interpretation['optimal_features_oos_r2']]\n",
    "})\n",
    "\n",
    "print(summary_stats.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "print(\"\\n✅ Overfitting analysis complete!\")\n",
    "print(f\"\\nOptimal model complexity (by Adjusted R²): {interpretation['optimal_features_adj_r2']} features\")\n",
    "print(f\"Optimal model complexity (by Out-of-Sample R²): {interpretation['optimal_features_oos_r2']} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Finally, let's save our results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "import os\n",
    "output_dir = '/home/runner/work/High_Dimensional_Linear_Models/High_Dimensional_Linear_Models/Python/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "results_df.to_csv(f'{output_dir}/overfitting_results.csv', index=False)\n",
    "print(f\"Results saved to {output_dir}/overfitting_results.csv\")\n",
    "\n",
    "# Also save summary statistics\n",
    "summary_stats.to_csv(f'{output_dir}/overfitting_summary.csv', index=False)\n",
    "print(f\"Summary statistics saved to {output_dir}/overfitting_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis has successfully demonstrated:\n",
    "\n",
    "1. **The bias-variance tradeoff**: As model complexity increases, we observe the classic pattern where out-of-sample performance first improves then deteriorates\n",
    "\n",
    "2. **The importance of proper model selection**: Adjusted R² provides a better guide than raw R² for choosing model complexity\n",
    "\n",
    "3. **The dangers of overfitting**: High-dimensional models can achieve perfect in-sample fit while performing terribly on new data\n",
    "\n",
    "4. **Practical implications**: This motivates the use of regularization techniques and cross-validation in machine learning\n",
    "\n",
    "The results clearly show why understanding overfitting is crucial for building models that generalize well to new data, particularly in high-dimensional settings common in modern econometrics and machine learning.\n",
    "\n",
    "This completes Part 2 of Assignment 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}