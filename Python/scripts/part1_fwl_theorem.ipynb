{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Part 1: Frisch-Waugh-Lovell (FWL) Theorem\n",
    "## Math (3 points)\n",
    "\n",
    "This notebook contains the mathematical proof and numerical verification of the Frisch-Waugh-Lovell theorem.\n",
    "\n",
    "The FWL theorem is a fundamental result in econometrics that shows how to isolate the effect of specific variables by \"partialling out\" the effects of other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Proof of the FWL Theorem\n",
    "\n",
    "The FWL theorem states that the OLS estimate of β₁ in the regression of y on [X₁ X₂] is equal to the OLS estimate obtained from the following two-step procedure:\n",
    "\n",
    "1. Regress y on X₂ and obtain the residuals ỹ = M_{X₂}y, where M_{X₂} = I - X₂(X₂'X₂)⁻¹X₂'\n",
    "2. Regress X₁ on X₂ and obtain the residuals X̃₁ = M_{X₂}X₁\n",
    "3. Regress ỹ on X̃₁ and show that the resulting coefficient vector is equal to β̂₁ from the full regression.\n",
    "\n",
    "Formally, we need to show that: β̂₁ = (X̃₁'X̃₁)⁻¹X̃₁'ỹ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FRISCH-WAUGH-LOVELL THEOREM PROOF ===\n",
      "\n",
      "Mathematical Proof:\n",
      "==================\n",
      "\n",
      "Consider the linear regression model:\n",
      "y = X₁β₁ + X₂β₂ + u\n",
      "\n",
      "Where:\n",
      "- y is an n×1 vector of outcomes\n",
      "- X₁ is an n×k₁ matrix of regressors of interest\n",
      "- X₂ is an n×k₂ matrix of control variables\n",
      "- u is an n×1 vector of errors\n",
      "\n",
      "Step 1: Full regression\n",
      "The full regression in matrix form is:\n",
      "y = [X₁ X₂][β₁; β₂] + u = Xβ + u\n",
      "\n",
      "The OLS estimator is:\n",
      "β̂ = (X'X)⁻¹X'y\n",
      "\n",
      "Partitioning X'X and X'y:\n",
      "X'X = [X₁'X₁  X₁'X₂]\n",
      "      [X₂'X₁  X₂'X₂]\n",
      "\n",
      "X'y = [X₁'y]\n",
      "      [X₂'y]\n",
      "\n",
      "Step 2: Using the partitioned inverse formula\n",
      "For a partitioned matrix [A B; C D], if D is invertible:\n",
      "The (1,1) block of the inverse is (A - BD⁻¹C)⁻¹\n",
      "\n",
      "Applying this to our case:\n",
      "β̂₁ = [(X₁'X₁ - X₁'X₂(X₂'X₂)⁻¹X₂'X₁)]⁻¹[X₁'y - X₁'X₂(X₂'X₂)⁻¹X₂'y]\n",
      "\n",
      "Step 3: Factoring out the projection matrix\n",
      "Let M_{X₂} = I - X₂(X₂'X₂)⁻¹X₂' (the annihilator matrix)\n",
      "Note that M_{X₂} is idempotent: M_{X₂}M_{X₂} = M_{X₂}\n",
      "And symmetric: M_{X₂}' = M_{X₂}\n",
      "\n",
      "Then:\n",
      "X₁'X₁ - X₁'X₂(X₂'X₂)⁻¹X₂'X₁ = X₁'[I - X₂(X₂'X₂)⁻¹X₂']X₁ = X₁'M_{X₂}X₁\n",
      "X₁'y - X₁'X₂(X₂'X₂)⁻¹X₂'y = X₁'[I - X₂(X₂'X₂)⁻¹X₂']y = X₁'M_{X₂}y\n",
      "\n",
      "Step 4: Final form\n",
      "Therefore:\n",
      "β̂₁ = (X₁'M_{X₂}X₁)⁻¹X₁'M_{X₂}y\n",
      "\n",
      "Let X̃₁ = M_{X₂}X₁ and ỹ = M_{X₂}y\n",
      "Then: β̂₁ = (X̃₁'X̃₁)⁻¹X̃₁'ỹ\n",
      "\n",
      "This shows that β̂₁ from the full regression equals the OLS coefficient\n",
      "from regressing the residuals ỹ on the residuals X̃₁.\n",
      "\n",
      "Q.E.D.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fwl_theorem_proof():\n",
    "    \"\"\"\n",
    "    Mathematical proof of the Frisch-Waugh-Lovell theorem.\n",
    "    \"\"\"\n",
    "    print(\"=== FRISCH-WAUGH-LOVELL THEOREM PROOF ===\\n\")\n",
    "    \n",
    "    print(\"Mathematical Proof:\")\n",
    "    print(\"==================\")\n",
    "    print()\n",
    "    print(\"Consider the linear regression model:\")\n",
    "    print(\"y = X₁β₁ + X₂β₂ + u\")\n",
    "    print()\n",
    "    print(\"Where:\")\n",
    "    print(\"- y is an n×1 vector of outcomes\")\n",
    "    print(\"- X₁ is an n×k₁ matrix of regressors of interest\")\n",
    "    print(\"- X₂ is an n×k₂ matrix of control variables\")\n",
    "    print(\"- u is an n×1 vector of errors\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Step 1: Full regression\")\n",
    "    print(\"The full regression in matrix form is:\")\n",
    "    print(\"y = [X₁ X₂][β₁; β₂] + u = Xβ + u\")\n",
    "    print()\n",
    "    print(\"The OLS estimator is:\")\n",
    "    print(\"β̂ = (X'X)⁻¹X'y\")\n",
    "    print()\n",
    "    print(\"Partitioning X'X and X'y:\")\n",
    "    print(\"X'X = [X₁'X₁  X₁'X₂]\")\n",
    "    print(\"      [X₂'X₁  X₂'X₂]\")\n",
    "    print()\n",
    "    print(\"X'y = [X₁'y]\")\n",
    "    print(\"      [X₂'y]\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Step 2: Using the partitioned inverse formula\")\n",
    "    print(\"For a partitioned matrix [A B; C D], if D is invertible:\")\n",
    "    print(\"The (1,1) block of the inverse is (A - BD⁻¹C)⁻¹\")\n",
    "    print()\n",
    "    print(\"Applying this to our case:\")\n",
    "    print(\"β̂₁ = [(X₁'X₁ - X₁'X₂(X₂'X₂)⁻¹X₂'X₁)]⁻¹[X₁'y - X₁'X₂(X₂'X₂)⁻¹X₂'y]\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Step 3: Factoring out the projection matrix\")\n",
    "    print(\"Let M_{X₂} = I - X₂(X₂'X₂)⁻¹X₂' (the annihilator matrix)\")\n",
    "    print(\"Note that M_{X₂} is idempotent: M_{X₂}M_{X₂} = M_{X₂}\")\n",
    "    print(\"And symmetric: M_{X₂}' = M_{X₂}\")\n",
    "    print()\n",
    "    print(\"Then:\")\n",
    "    print(\"X₁'X₁ - X₁'X₂(X₂'X₂)⁻¹X₂'X₁ = X₁'[I - X₂(X₂'X₂)⁻¹X₂']X₁ = X₁'M_{X₂}X₁\")\n",
    "    print(\"X₁'y - X₁'X₂(X₂'X₂)⁻¹X₂'y = X₁'[I - X₂(X₂'X₂)⁻¹X₂']y = X₁'M_{X₂}y\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Step 4: Final form\")\n",
    "    print(\"Therefore:\")\n",
    "    print(\"β̂₁ = (X₁'M_{X₂}X₁)⁻¹X₁'M_{X₂}y\")\n",
    "    print()\n",
    "    print(\"Let X̃₁ = M_{X₂}X₁ and ỹ = M_{X₂}y\")\n",
    "    print(\"Then: β̂₁ = (X̃₁'X̃₁)⁻¹X̃₁'ỹ\")\n",
    "    print()\n",
    "    print(\"This shows that β̂₁ from the full regression equals the OLS coefficient\")\n",
    "    print(\"from regressing the residuals ỹ on the residuals X̃₁.\")\n",
    "    print()\n",
    "    print(\"Q.E.D.\")\n",
    "    print()\n",
    "\n",
    "# Display the mathematical proof\n",
    "fwl_theorem_proof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Verification\n",
    "\n",
    "Now let's verify the FWL theorem numerically using simulated data. We'll generate data with known parameters and compare the results from:\n",
    "1. Full regression: y ~ [X₁ X₂]\n",
    "2. FWL two-step procedure: residuals of y on residuals of X₁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NUMERICAL VERIFICATION ===\n",
      "\n",
      "Sample size: 1000\n",
      "X1 dimensions: (1000, 2) (variables of interest)\n",
      "X2 dimensions: (1000, 3) (control variables)\n",
      "True β₁: [1.5 2. ]\n",
      "True β₂: [ 0.5 -1.   0.8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def numerical_verification():\n",
    "    \"\"\"\n",
    "    Numerical verification of the FWL theorem using simulated data.\n",
    "    \"\"\"\n",
    "    print(\"=== NUMERICAL VERIFICATION ===\\n\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate data\n",
    "    n = 1000  # Sample size\n",
    "    k1 = 2    # Number of variables of interest\n",
    "    k2 = 3    # Number of control variables\n",
    "    \n",
    "    # Generate X1, X2, and error term\n",
    "    X1 = np.random.randn(n, k1)\n",
    "    X2 = np.random.randn(n, k2)\n",
    "    u = np.random.randn(n, 1)\n",
    "    \n",
    "    # True parameters\n",
    "    beta1_true = np.array([[1.5], [2.0]])\n",
    "    beta2_true = np.array([[0.5], [-1.0], [0.8]])\n",
    "    \n",
    "    # Generate y\n",
    "    y = X1 @ beta1_true + X2 @ beta2_true + u\n",
    "    \n",
    "    print(f\"Sample size: {n}\")\n",
    "    print(f\"X1 dimensions: {X1.shape} (variables of interest)\")\n",
    "    print(f\"X2 dimensions: {X2.shape} (control variables)\")\n",
    "    print(f\"True β₁: {beta1_true.ravel()}\")\n",
    "    print(f\"True β₂: {beta2_true.ravel()}\")\n",
    "    print()\n",
    "    \n",
    "    return X1, X2, y, beta1_true, beta2_true, n, k1, k2\n",
    "\n",
    "# Generate the data\n",
    "X1, X2, y, beta1_true, beta2_true, n, k1, k2 = numerical_verification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Full Regression\n",
    "\n",
    "First, let's estimate the full regression model with all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Full regression\n",
      "β̂₁ from full regression: [1.52360737 1.99063613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Full regression\n",
    "X_full = np.column_stack([X1, X2])\n",
    "beta_full = np.linalg.inv(X_full.T @ X_full) @ X_full.T @ y\n",
    "beta1_full = beta_full[:k1]\n",
    "\n",
    "print(\"Method 1: Full regression\")\n",
    "print(f\"β̂₁ from full regression: {beta1_full.ravel()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: FWL Two-Step Procedure\n",
    "\n",
    "Now let's implement the FWL two-step procedure:\n",
    "1. Residualize y and X₁ with respect to X₂\n",
    "2. Regress the residualized y on the residualized X₁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 2: FWL two-step procedure\n",
      "Step 1: Residualize y on X₂\n",
      "Step 2: Residualize X₁ on X₂\n",
      "Step 3: Regress residuals\n",
      "β̂₁ from FWL method: [1.52360737 1.99063613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 2: FWL two-step procedure\n",
    "\n",
    "# Step 1: Regress y on X2 and get residuals\n",
    "P_X2 = X2 @ np.linalg.inv(X2.T @ X2) @ X2.T\n",
    "M_X2 = np.eye(n) - P_X2\n",
    "y_tilde = M_X2 @ y\n",
    "\n",
    "# Step 2: Regress X1 on X2 and get residuals\n",
    "X1_tilde = M_X2 @ X1\n",
    "\n",
    "# Step 3: Regress y_tilde on X1_tilde\n",
    "beta1_fwl = np.linalg.inv(X1_tilde.T @ X1_tilde) @ X1_tilde.T @ y_tilde\n",
    "\n",
    "print(\"Method 2: FWL two-step procedure\")\n",
    "print(f\"Step 1: Residualize y on X₂\")\n",
    "print(f\"Step 2: Residualize X₁ on X₂\")\n",
    "print(f\"Step 3: Regress residuals\")\n",
    "print(f\"β̂₁ from FWL method: {beta1_fwl.ravel()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison and Verification\n",
    "\n",
    "Let's check if both methods produce identical results (within numerical precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification:\n",
      "Maximum absolute difference: 4.44e-16\n",
      "Are they equal (within 1e-10)? True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if they are equal (within numerical precision)\n",
    "difference = np.abs(beta1_full - beta1_fwl)\n",
    "max_diff = np.max(difference)\n",
    "\n",
    "print(\"Verification:\")\n",
    "print(f\"Maximum absolute difference: {max_diff:.2e}\")\n",
    "print(f\"Are they equal (within 1e-10)? {max_diff < 1e-10}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Verification using Scikit-learn\n",
    "\n",
    "Let's also verify using scikit-learn to ensure our manual implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative verification using sklearn:\n",
      "β̂₁ from sklearn full regression: [1.52360737 1.99063613]\n",
      "β̂₁ from sklearn FWL method: [1.52360737 1.99063613]\n",
      "Maximum absolute difference (sklearn): 2.00e-15\n",
      "Are they equal (within 1e-10)? True\n"
     ]
    }
   ],
   "source": [
    "# Alternative verification using sklearn\n",
    "print(\"Alternative verification using sklearn:\")\n",
    "\n",
    "# Full regression with sklearn\n",
    "reg_full = LinearRegression(fit_intercept=False)\n",
    "reg_full.fit(X_full, y.ravel())\n",
    "beta1_sklearn_full = reg_full.coef_[:k1]\n",
    "\n",
    "# FWL with sklearn\n",
    "# Step 1: Get residuals y_tilde\n",
    "reg_y_on_x2 = LinearRegression(fit_intercept=False)\n",
    "reg_y_on_x2.fit(X2, y.ravel())\n",
    "y_tilde_sklearn = y.ravel() - reg_y_on_x2.predict(X2)\n",
    "\n",
    "# Step 2: Get residuals X1_tilde\n",
    "X1_tilde_sklearn = np.zeros_like(X1)\n",
    "for i in range(k1):\n",
    "    reg_x1_on_x2 = LinearRegression(fit_intercept=False)\n",
    "    reg_x1_on_x2.fit(X2, X1[:, i])\n",
    "    X1_tilde_sklearn[:, i] = X1[:, i] - reg_x1_on_x2.predict(X2)\n",
    "\n",
    "# Step 3: Final regression\n",
    "reg_fwl = LinearRegression(fit_intercept=False)\n",
    "reg_fwl.fit(X1_tilde_sklearn, y_tilde_sklearn)\n",
    "beta1_sklearn_fwl = reg_fwl.coef_\n",
    "\n",
    "print(f\"β̂₁ from sklearn full regression: {beta1_sklearn_full}\")\n",
    "print(f\"β̂₁ from sklearn FWL method: {beta1_sklearn_fwl}\")\n",
    "\n",
    "diff_sklearn = np.abs(beta1_sklearn_full - beta1_sklearn_fwl)\n",
    "max_diff_sklearn = np.max(diff_sklearn)\n",
    "print(f\"Maximum absolute difference (sklearn): {max_diff_sklearn:.2e}\")\n",
    "print(f\"Are they equal (within 1e-10)? {max_diff_sklearn < 1e-10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Results\n",
    "\n",
    "Let's create a summary table of all our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESULTS SUMMARY ===\n",
      "                   Method    β₁[0]    β₁[1]\n",
      "              True Values 1.500000 2.000000\n",
      " Full Regression (Manual) 1.523607 1.990636\n",
      "      FWL Method (Manual) 1.523607 1.990636\n",
      "Full Regression (Sklearn) 1.523607 1.990636\n",
      "     FWL Method (Sklearn) 1.523607 1.990636\n",
      "\n",
      "Maximum difference between methods: 2.00e-15\n",
      "\n",
      "✅ FWL Theorem verification SUCCESSFUL!\n",
      "The full regression and FWL two-step procedure produce identical results.\n"
     ]
    }
   ],
   "source": [
    "# Create results summary\n",
    "results_df = pd.DataFrame({\n",
    "    'Method': ['True Values', 'Full Regression (Manual)', 'FWL Method (Manual)', \n",
    "               'Full Regression (Sklearn)', 'FWL Method (Sklearn)'],\n",
    "    'β₁[0]': [beta1_true[0,0], beta1_full[0,0], beta1_fwl[0,0], \n",
    "              beta1_sklearn_full[0], beta1_sklearn_fwl[0]],\n",
    "    'β₁[1]': [beta1_true[1,0], beta1_full[1,0], beta1_fwl[1,0], \n",
    "              beta1_sklearn_full[1], beta1_sklearn_fwl[1]]\n",
    "})\n",
    "\n",
    "print(\"\\n=== RESULTS SUMMARY ===\")\n",
    "print(results_df.to_string(index=False, float_format='%.6f'))\n",
    "\n",
    "print(f\"\\nMaximum difference between methods: {max(max_diff, max_diff_sklearn):.2e}\")\n",
    "print(\"\\n✅ FWL Theorem verification SUCCESSFUL!\")\n",
    "print(\"The full regression and FWL two-step procedure produce identical results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have successfully:\n",
    "\n",
    "1. **Provided a complete mathematical proof** of the Frisch-Waugh-Lovell theorem using partitioned matrix algebra\n",
    "2. **Numerically verified** the theorem using simulated data with both manual implementations and scikit-learn\n",
    "3. **Demonstrated** that both the full regression and the FWL two-step procedure produce identical estimates (within machine precision)\n",
    "\n",
    "The FWL theorem is a powerful tool in econometrics that allows us to:\n",
    "- Isolate the effect of specific variables by \"partialling out\" control variables\n",
    "- Understand the mechanics of multiple regression\n",
    "- Implement efficient computational methods for large datasets\n",
    "\n",
    "This completes Part 1 of Assignment 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
