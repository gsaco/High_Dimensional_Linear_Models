{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Dimensional Linear Models: Overfitting Simulation (Julia)\n",
    "\n",
    "This notebook demonstrates the overfitting phenomenon in high-dimensional linear models using Julia.\n",
    "We'll generate data with a nonlinear relationship and fit linear models with increasing numbers of polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, Distributions, LinearAlgebra\n",
    "using DataFrames, CSV\n",
    "using Plots, StatsPlots\n",
    "using GLM, StatsBase\n",
    "using MLBase\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "Random.seed!(42)\n",
    "println(\"Packages loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generating Process\n",
    "\n",
    "We use the specified data generating process:\n",
    "- f_X = exp(4 * X) - 1\n",
    "- y = f_X + ε, where ε ~ N(0, σ²)\n",
    "- n = 1000 observations\n",
    "- Intercept = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n = 1000\n",
    "noise_std = 0.5  # Standard deviation of the error term\n",
    "\n",
    "# Generate X from uniform distribution\n",
    "X = rand(Uniform(-0.5, 0.5), n)\n",
    "\n",
    "# Data generating process: f_X = exp(4 * X) - 1\n",
    "f_X = exp.(4 * X) .- 1\n",
    "\n",
    "# Add noise to get Y\n",
    "epsilon = rand(Normal(0, noise_std), n)\n",
    "Y = f_X + epsilon\n",
    "\n",
    "println(\"Generated $n observations\")\n",
    "println(\"X range: [$(minimum(X)), $(maximum(X))]\")\n",
    "println(\"Y range: [$(minimum(Y)), $(maximum(Y))]\")\n",
    "println(\"True function range: [$(minimum(f_X)), $(maximum(f_X))]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the True Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the true relationship\n",
    "X_sorted = sort(X)\n",
    "f_sorted = exp.(4 * X_sorted) .- 1\n",
    "\n",
    "p1 = scatter(X, Y, alpha=0.5, ms=2, label=\"Observed data\", \n",
    "            xlabel=\"X\", ylabel=\"Y\", title=\"Data Generating Process\")\n",
    "plot!(p1, X_sorted, f_sorted, linewidth=2, color=:red, \n",
    "      label=\"True function: exp(4X) - 1\")\n",
    "display(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Model Fitting and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_polynomial_features(x, degree)\n",
    "    \"\"\"Create polynomial features up to specified degree\"\"\"\n",
    "    n = length(x)\n",
    "    X_poly = zeros(n, degree)\n",
    "    \n",
    "    for i in 1:degree\n",
    "        X_poly[:, i] = x .^ i\n",
    "    end\n",
    "    \n",
    "    return X_poly\n",
    "end\n",
    "\n",
    "function calculate_r2(y_true, y_pred)\n",
    "    \"\"\"Calculate R-squared\"\"\"\n",
    "    ss_res = sum((y_true - y_pred).^2)\n",
    "    ss_tot = sum((y_true .- mean(y_true)).^2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "end\n",
    "\n",
    "function calculate_adjusted_r2(r2, n, p)\n",
    "    \"\"\"Calculate adjusted R-squared\"\"\"\n",
    "    if n <= p + 1\n",
    "        return NaN\n",
    "    end\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "end\n",
    "\n",
    "function fit_and_evaluate(X_train, y_train, X_test, y_test, n_features)\n",
    "    \"\"\"Fit polynomial regression model and calculate metrics\"\"\"\n",
    "    try\n",
    "        # Create polynomial features\n",
    "        X_train_poly = create_polynomial_features(X_train, n_features)\n",
    "        X_test_poly = create_polynomial_features(X_test, n_features)\n",
    "        \n",
    "        # Fit linear regression without intercept\n",
    "        # Using normal equations: β = (X'X)^(-1)X'y\n",
    "        coeffs = (X_train_poly' * X_train_poly) \\ (X_train_poly' * y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = X_train_poly * coeffs\n",
    "        y_test_pred = X_test_poly * coeffs\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2_train = calculate_r2(y_train, y_train_pred)\n",
    "        r2_test = calculate_r2(y_test, y_test_pred)\n",
    "        adj_r2 = calculate_adjusted_r2(r2_train, length(y_train), n_features)\n",
    "        \n",
    "        return Dict(\n",
    "            \"r2\" => r2_train,\n",
    "            \"adj_r2\" => adj_r2,\n",
    "            \"r2_oos\" => r2_test,\n",
    "            \"n_params\" => n_features,\n",
    "            \"success\" => true\n",
    "        )\n",
    "    catch e\n",
    "        return Dict(\n",
    "            \"r2\" => NaN,\n",
    "            \"adj_r2\" => NaN,\n",
    "            \"r2_oos\" => NaN,\n",
    "            \"n_params\" => n_features,\n",
    "            \"success\" => false,\n",
    "            \"error\" => string(e)\n",
    "        )\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Simulation Loop\n",
    "\n",
    "We'll test models with different numbers of polynomial features: 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train (75%) and test (25%)\n",
    "train_size = Int(floor(0.75 * n))\n",
    "indices = randperm(n)\n",
    "train_idx = indices[1:train_size]\n",
    "test_idx = indices[train_size+1:end]\n",
    "\n",
    "X_train = X[train_idx]\n",
    "y_train = Y[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_test = Y[test_idx]\n",
    "\n",
    "println(\"Training set size: $(length(X_train))\")\n",
    "println(\"Test set size: $(length(X_test))\")\n",
    "\n",
    "# Number of features to test\n",
    "feature_counts = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "# Storage for results\n",
    "results = DataFrame(\n",
    "    n_features = Int[],\n",
    "    r2 = Float64[],\n",
    "    adj_r2 = Float64[],\n",
    "    r2_oos = Float64[],\n",
    "    n_params = Int[]\n",
    ")\n",
    "\n",
    "println(\"\\nRunning simulation...\")\n",
    "println(\"Features | R² (train) | Adj R² | R² (test) | Parameters\")\n",
    "println(\"-\"^60)\n",
    "\n",
    "for n_features in feature_counts\n",
    "    # Skip if we don't have enough training samples\n",
    "    if n_features >= length(X_train)\n",
    "        println(\"$(lpad(n_features, 8)) | Skipped (insufficient training data)\")\n",
    "        continue\n",
    "    end\n",
    "    \n",
    "    metrics = fit_and_evaluate(X_train, y_train, X_test, y_test, n_features)\n",
    "    \n",
    "    if metrics[\"success\"]\n",
    "        push!(results, (\n",
    "            n_features = n_features,\n",
    "            r2 = metrics[\"r2\"],\n",
    "            adj_r2 = metrics[\"adj_r2\"],\n",
    "            r2_oos = metrics[\"r2_oos\"],\n",
    "            n_params = metrics[\"n_params\"]\n",
    "        ))\n",
    "        \n",
    "        println(\"$(lpad(n_features, 8)) | $(rpad(round(metrics[\"r2\"], digits=4), 9)) | $(rpad(round(metrics[\"adj_r2\"], digits=4), 6)) | $(rpad(round(metrics[\"r2_oos\"], digits=4), 8)) | $(lpad(metrics[\"n_params\"], 9))\")\n",
    "    else\n",
    "        println(\"$(lpad(n_features, 8)) | Error: $(metrics[\"error\"][1:min(30, end)])...\")\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"\\nCompleted simulation with $(nrow(results)) successful models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "We create three separate plots showing how the different R-squared metrics change with the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the three plots\n",
    "p1 = plot(results.n_features, results.r2, \n",
    "         marker=:circle, linewidth=2, markersize=6,\n",
    "         xlabel=\"Number of Features\", ylabel=\"R-squared (Training)\",\n",
    "         title=\"Training R-squared vs Number of Features\",\n",
    "         xscale=:log10, ylims=(0, 1), grid=true, color=:blue)\n",
    "\n",
    "# Filter out NaN values for adjusted R²\n",
    "valid_adj = filter(row -> !isnan(row.adj_r2), results)\n",
    "p2 = plot(valid_adj.n_features, valid_adj.adj_r2, \n",
    "         marker=:circle, linewidth=2, markersize=6,\n",
    "         xlabel=\"Number of Features\", ylabel=\"Adjusted R-squared\",\n",
    "         title=\"Adjusted R-squared vs Number of Features\",\n",
    "         xscale=:log10, grid=true, color=:green)\n",
    "\n",
    "p3 = plot(results.n_features, results.r2_oos, \n",
    "         marker=:circle, linewidth=2, markersize=6,\n",
    "         xlabel=\"Number of Features\", ylabel=\"Out-of-sample R-squared\",\n",
    "         title=\"Out-of-sample R-squared vs Number of Features\",\n",
    "         xscale=:log10, grid=true, color=:red)\n",
    "\n",
    "# Combine plots\n",
    "combined_plot = plot(p1, p2, p3, layout=(1,3), size=(1200, 400))\n",
    "display(combined_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Analysis\n",
    "\n",
    "Let's examine the results and discuss the overfitting phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "println(\"Summary of Results:\")\n",
    "println(\"=\"^80)\n",
    "show(results, allrows=true)\n",
    "\n",
    "# Find optimal number of features based on out-of-sample R²\n",
    "best_idx = argmax(results.r2_oos)\n",
    "best_n_features = results.n_features[best_idx]\n",
    "best_oos_r2 = results.r2_oos[best_idx]\n",
    "\n",
    "println(\"\\n\\nOptimal number of features (based on out-of-sample R²): $best_n_features\")\n",
    "println(\"Best out-of-sample R²: $(round(best_oos_r2, digits=4))\")\n",
    "\n",
    "# Calculate the difference between training and test R² to show overfitting\n",
    "results.overfitting = results.r2 - results.r2_oos\n",
    "println(\"\\nOverfitting Analysis (Training R² - Test R²):\")\n",
    "for i in 1:nrow(results)\n",
    "    println(\"$(lpad(results.n_features[i], 8)) | $(rpad(round(results.overfitting[i], digits=4), 8))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation and Conclusions\n",
    "\n",
    "### Overfitting Demonstration\n",
    "\n",
    "This simulation clearly demonstrates the overfitting phenomenon in high-dimensional linear models:\n",
    "\n",
    "1. **Training R-squared** monotonically increases as we add more polynomial features. This makes sense because with more parameters, the model can fit the training data more closely.\n",
    "\n",
    "2. **Adjusted R-squared** initially increases but then starts to decrease as the penalty for additional parameters outweighs the improvement in fit. This metric tries to balance model fit with model complexity.\n",
    "\n",
    "3. **Out-of-sample R-squared** typically increases initially as we capture more of the true nonlinear relationship, but then decreases as the model becomes too complex and starts fitting noise rather than signal.\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Bias-Variance Tradeoff**: Simple models (few features) have high bias but low variance. Complex models (many features) have low bias but high variance.\n",
    "- **Optimal Complexity**: There's an optimal number of features that maximizes out-of-sample performance.\n",
    "- **Generalization**: Models that perform well on training data don't necessarily generalize well to new data.\n",
    "\n",
    "### Julia-Specific Observations:\n",
    "\n",
    "- Julia's linear algebra capabilities make matrix operations efficient\n",
    "- The language's mathematical syntax closely matches the theoretical formulations\n",
    "- Broadcasting operations (`.` syntax) make vectorized computations intuitive\n",
    "\n",
    "### Practical Implications:\n",
    "\n",
    "- Always use cross-validation or hold-out samples to evaluate model performance\n",
    "- Consider regularization techniques for high-dimensional problems\n",
    "- Be cautious of models with very high training accuracy but poor test performance\n",
    "- The true data generating process is nonlinear (exponential), but we're using polynomial approximations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}