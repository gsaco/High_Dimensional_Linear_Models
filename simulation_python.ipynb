{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Dimensional Linear Models: Overfitting Simulation (Python)\n",
    "\n",
    "This notebook demonstrates the overfitting phenomenon in high-dimensional linear models.\n",
    "We'll generate data with a nonlinear relationship and fit linear models with increasing numbers of polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generating Process\n",
    "\n",
    "We use the specified data generating process:\n",
    "- f_X = exp(4 * X) - 1\n",
    "- y = f_X + ε, where ε ~ N(0, σ²)\n",
    "- n = 1000 observations\n",
    "- Intercept = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n = 1000\n",
    "noise_std = 0.5  # Standard deviation of the error term\n",
    "\n",
    "# Generate X from uniform distribution\n",
    "X = np.random.uniform(-0.5, 0.5, n)\n",
    "\n",
    "# Data generating process: f_X = exp(4 * X) - 1\n",
    "f_X = np.exp(4 * X) - 1\n",
    "\n",
    "# Add noise to get Y\n",
    "epsilon = np.random.normal(0, noise_std, n)\n",
    "Y = f_X + epsilon\n",
    "\n",
    "print(f\"Generated {n} observations\")\n",
    "print(f\"X range: [{X.min():.3f}, {X.max():.3f}]\")\n",
    "print(f\"Y range: [{Y.min():.3f}, {Y.max():.3f}]\")\n",
    "print(f\"True function range: [{f_X.min():.3f}, {f_X.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the True Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the true relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, Y, alpha=0.5, s=20, label='Observed data')\n",
    "X_sorted = np.sort(X)\n",
    "f_sorted = np.exp(4 * X_sorted) - 1\n",
    "plt.plot(X_sorted, f_sorted, 'r-', linewidth=2, label='True function: exp(4X) - 1')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Data Generating Process')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Calculate R-squared Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adjusted_r2(r2, n, p):\n",
    "    \"\"\"\n",
    "    Calculate adjusted R-squared\n",
    "    \n",
    "    Parameters:\n",
    "    r2: R-squared value\n",
    "    n: number of observations\n",
    "    p: number of predictors (excluding intercept)\n",
    "    \"\"\"\n",
    "    if n <= p + 1:\n",
    "        return np.nan\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "def fit_and_evaluate(X_train, y_train, X_test, y_test, n_features):\n",
    "    \"\"\"\n",
    "    Fit polynomial regression model and calculate metrics\n",
    "    \n",
    "    Parameters:\n",
    "    X_train, y_train: training data\n",
    "    X_test, y_test: test data  \n",
    "    n_features: number of polynomial features to include\n",
    "    \n",
    "    Returns:\n",
    "    dict with R-squared, adjusted R-squared, and out-of-sample R-squared\n",
    "    \"\"\"\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=n_features, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train.reshape(-1, 1))\n",
    "    X_test_poly = poly.transform(X_test.reshape(-1, 1))\n",
    "    \n",
    "    # Fit linear regression (without intercept as specified)\n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_poly)\n",
    "    y_test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    adj_r2 = calculate_adjusted_r2(r2_train, len(y_train), X_train_poly.shape[1])\n",
    "    \n",
    "    return {\n",
    "        'r2': r2_train,\n",
    "        'adj_r2': adj_r2,\n",
    "        'r2_oos': r2_test,\n",
    "        'n_params': X_train_poly.shape[1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Simulation Loop\n",
    "\n",
    "We'll test models with different numbers of polynomial features: 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train (75%) and test (25%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Number of features to test\n",
    "feature_counts = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "# Storage for results\n",
    "results = []\n",
    "\n",
    "print(\"\\nRunning simulation...\")\n",
    "print(\"Features | R² (train) | Adj R² | R² (test) | Parameters\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for n_features in feature_counts:\n",
    "    # Skip if we don't have enough training samples\n",
    "    if n_features >= len(X_train):\n",
    "        print(f\"{n_features:8d} | Skipped (insufficient training data)\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        metrics = fit_and_evaluate(X_train, y_train, X_test, y_test, n_features)\n",
    "        \n",
    "        results.append({\n",
    "            'n_features': n_features,\n",
    "            'r2': metrics['r2'],\n",
    "            'adj_r2': metrics['adj_r2'],\n",
    "            'r2_oos': metrics['r2_oos'],\n",
    "            'n_params': metrics['n_params']\n",
    "        })\n",
    "        \n",
    "        print(f\"{n_features:8d} | {metrics['r2']:9.4f} | {metrics['adj_r2']:6.4f} | {metrics['r2_oos']:8.4f} | {metrics['n_params']:9d}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{n_features:8d} | Error: {str(e)[:30]}...\")\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nCompleted simulation with {len(results_df)} successful models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "We create three separate plots showing how the different R-squared metrics change with the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the three plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: R-squared (training)\n",
    "axes[0].plot(results_df['n_features'], results_df['r2'], 'bo-', linewidth=2, markersize=6)\n",
    "axes[0].set_xlabel('Number of Features')\n",
    "axes[0].set_ylabel('R-squared (Training)')\n",
    "axes[0].set_title('Training R-squared vs Number of Features')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Adjusted R-squared\n",
    "valid_adj = results_df.dropna(subset=['adj_r2'])\n",
    "axes[1].plot(valid_adj['n_features'], valid_adj['adj_r2'], 'go-', linewidth=2, markersize=6)\n",
    "axes[1].set_xlabel('Number of Features')\n",
    "axes[1].set_ylabel('Adjusted R-squared')\n",
    "axes[1].set_title('Adjusted R-squared vs Number of Features')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "# Plot 3: Out-of-sample R-squared\n",
    "axes[2].plot(results_df['n_features'], results_df['r2_oos'], 'ro-', linewidth=2, markersize=6)\n",
    "axes[2].set_xlabel('Number of Features')\n",
    "axes[2].set_ylabel('Out-of-sample R-squared')\n",
    "axes[2].set_title('Out-of-sample R-squared vs Number of Features')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Analysis\n",
    "\n",
    "Let's examine the results and discuss the overfitting phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"Summary of Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Find optimal number of features based on out-of-sample R²\n",
    "best_oos_idx = results_df['r2_oos'].idxmax()\n",
    "best_n_features = results_df.loc[best_oos_idx, 'n_features']\n",
    "best_oos_r2 = results_df.loc[best_oos_idx, 'r2_oos']\n",
    "\n",
    "print(f\"\\nOptimal number of features (based on out-of-sample R²): {best_n_features}\")\n",
    "print(f\"Best out-of-sample R²: {best_oos_r2:.4f}\")\n",
    "\n",
    "# Calculate the difference between training and test R² to show overfitting\n",
    "results_df['overfitting'] = results_df['r2'] - results_df['r2_oos']\n",
    "print(f\"\\nOverfitting Analysis (Training R² - Test R²):\")\n",
    "print(results_df[['n_features', 'overfitting']].to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation and Conclusions\n",
    "\n",
    "### Overfitting Demonstration\n",
    "\n",
    "This simulation clearly demonstrates the overfitting phenomenon in high-dimensional linear models:\n",
    "\n",
    "1. **Training R-squared** monotonically increases as we add more polynomial features. This makes sense because with more parameters, the model can fit the training data more closely.\n",
    "\n",
    "2. **Adjusted R-squared** initially increases but then starts to decrease as the penalty for additional parameters outweighs the improvement in fit. This metric tries to balance model fit with model complexity.\n",
    "\n",
    "3. **Out-of-sample R-squared** typically increases initially as we capture more of the true nonlinear relationship, but then decreases as the model becomes too complex and starts fitting noise rather than signal.\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Bias-Variance Tradeoff**: Simple models (few features) have high bias but low variance. Complex models (many features) have low bias but high variance.\n",
    "- **Optimal Complexity**: There's an optimal number of features that maximizes out-of-sample performance.\n",
    "- **Generalization**: Models that perform well on training data don't necessarily generalize well to new data.\n",
    "\n",
    "### Practical Implications:\n",
    "\n",
    "- Always use cross-validation or hold-out samples to evaluate model performance\n",
    "- Consider regularization techniques (Ridge, Lasso) for high-dimensional problems\n",
    "- Be cautious of models with very high training accuracy but poor test performance\n",
    "- The true data generating process is nonlinear (exponential), but we're using polynomial approximations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}