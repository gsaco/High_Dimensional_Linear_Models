{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Part 2: Overfitting Analysis\n",
    "## Overfitting (8 points)\n",
    "\n",
    "This notebook simulates a data generating process and analyzes overfitting by estimating linear models with increasing numbers of polynomial features using R.\n",
    "\n",
    "We will demonstrate the classic bias-variance tradeoff by examining how different R-squared measures behave as model complexity increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "\n",
    "# Set options for better output display\n",
    "options(digits = 6)\n",
    "options(scipen = 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We'll generate data following a simple relationship: y = 2X + u, where u is random noise. This gives us a known ground truth to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "generate_data <- function(n = 1000, seed = 42) {\n",
    "  #' Generate data following the specification with only 2 variables X and Y.\n",
    "  #' Intercept parameter is set to zero as requested.\n",
    "  #'\n",
    "  #' @param n Sample size (default: 1000)\n",
    "  #' @param seed Random seed for reproducibility\n",
    "  #'\n",
    "  #' @return List containing X (feature matrix) and y (target variable)\n",
    "  \n",
    "  set.seed(seed)\n",
    "  \n",
    "  # Generate X (single feature initially)\n",
    "  X <- matrix(rnorm(n), nrow = n, ncol = 1)\n",
    "  \n",
    "  # Generate error term\n",
    "  u <- rnorm(n)\n",
    "  \n",
    "  # Generate y with no intercept (as requested)\n",
    "  # True relationship: y = 2*X + u\n",
    "  beta_true <- 2.0\n",
    "  y <- beta_true * X[, 1] + u\n",
    "  \n",
    "  return(list(X = X, y = y))\n",
    "}\n",
    "\n",
    "# Generate the data\n",
    "data <- generate_data(n = 1000, seed = 42)\n",
    "X <- data$X\n",
    "y <- data$y\n",
    "\n",
    "cat(sprintf(\"Generated data with n=%d observations\\n\", length(y)))\n",
    "cat(\"True relationship: y = 2*X + u\\n\")\n",
    "cat(sprintf(\"X shape: (%d, %d)\\n\", nrow(X), ncol(X)))\n",
    "cat(sprintf(\"y length: %d\\n\", length(y)))\n",
    "cat(sprintf(\"X mean: %.4f, X std: %.4f\\n\", mean(X), sd(X)))\n",
    "cat(sprintf(\"y mean: %.4f, y std: %.4f\\n\", mean(y), sd(y)))\n",
    "\n",
    "# Display first few observations\n",
    "cat(\"\\nFirst 10 observations:\\n\")\n",
    "head_data <- data.frame(X = X[1:10, 1], y = y[1:10])\n",
    "print(head_data, row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's define helper functions for polynomial feature creation, adjusted R-squared calculation, and data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "create_polynomial_features <- function(X, n_features) {\n",
    "  #' Create polynomial features up to n_features.\n",
    "  #'\n",
    "  #' @param X Original feature matrix (n x 1)\n",
    "  #' @param n_features Number of features to create\n",
    "  #'\n",
    "  #' @return Extended feature matrix with polynomial features\n",
    "  \n",
    "  n_samples <- nrow(X)\n",
    "  X_poly <- matrix(0, nrow = n_samples, ncol = n_features)\n",
    "  \n",
    "  for (i in 1:n_features) {\n",
    "    X_poly[, i] <- X[, 1]^i  # x^1, x^2, x^3, etc.\n",
    "  }\n",
    "  \n",
    "  return(X_poly)\n",
    "}\n",
    "\n",
    "calculate_adjusted_r2 <- function(r2, n, k) {\n",
    "  #' Calculate adjusted R-squared.\n",
    "  #'\n",
    "  #' Adjusted R² = 1 - [(1 - R²)(n - 1) / (n - k - 1)]\n",
    "  #'\n",
    "  #' @param r2 R-squared value\n",
    "  #' @param n Sample size\n",
    "  #' @param k Number of features (excluding intercept)\n",
    "  #'\n",
    "  #' @return Adjusted R-squared\n",
    "  \n",
    "  if (n - k - 1 <= 0) {\n",
    "    return(NA)\n",
    "  }\n",
    "  \n",
    "  adj_r2 <- 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "  return(adj_r2)\n",
    "}\n",
    "\n",
    "r2_score <- function(y_true, y_pred) {\n",
    "  #' Calculate R-squared score.\n",
    "  ss_res <- sum((y_true - y_pred)^2)\n",
    "  ss_tot <- sum((y_true - mean(y_true))^2)\n",
    "  return(1 - (ss_res / ss_tot))\n",
    "}\n",
    "\n",
    "train_test_split <- function(X, y, test_size = 0.25, random_state = 42) {\n",
    "  #' Split data into training and testing sets.\n",
    "  set.seed(random_state)\n",
    "  n <- length(y)\n",
    "  n_test <- round(n * test_size)\n",
    "  indices <- sample(1:n, n)\n",
    "  \n",
    "  test_indices <- indices[1:n_test]\n",
    "  train_indices <- indices[(n_test + 1):n]\n",
    "  \n",
    "  return(list(\n",
    "    X_train = X[train_indices, , drop = FALSE],\n",
    "    X_test = X[test_indices, , drop = FALSE],\n",
    "    y_train = y[train_indices],\n",
    "    y_test = y[test_indices]\n",
    "  ))\n",
    "}\n",
    "\n",
    "# Example: create polynomial features\n",
    "X_poly_example <- create_polynomial_features(X, 5)\n",
    "cat(sprintf(\"Example: Original X shape: (%d, %d)\\n\", nrow(X), ncol(X)))\n",
    "cat(sprintf(\"Example: Polynomial features (5 features) shape: (%d, %d)\\n\", nrow(X_poly_example), ncol(X_poly_example)))\n",
    "cat(\"First 5 rows of polynomial features:\\n\")\n",
    "print(X_poly_example[1:5, ], digits = 4)\n",
    "\n",
    "# Example: adjusted R-squared calculation\n",
    "example_r2 <- 0.8\n",
    "example_n <- 1000\n",
    "example_k <- 5\n",
    "example_adj_r2 <- calculate_adjusted_r2(example_r2, example_n, example_k)\n",
    "cat(sprintf(\"\\nExample: R² = %.1f, n = %d, k = %d\\n\", example_r2, example_n, example_k))\n",
    "cat(sprintf(\"Adjusted R² = %.4f\\n\", example_adj_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Analysis\n",
    "\n",
    "Now we'll perform the main analysis, testing models with different numbers of polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "overfitting_analysis <- function() {\n",
    "  #' Main function to perform overfitting analysis.\n",
    "  \n",
    "  cat(\"=== OVERFITTING ANALYSIS ===\\n\\n\")\n",
    "  \n",
    "  # Number of features to test\n",
    "  n_features_list <- c(1, 2, 5, 10, 20, 50, 100, 200, 500, 1000)\n",
    "  \n",
    "  # Storage for results\n",
    "  results <- data.frame(\n",
    "    n_features = integer(),\n",
    "    r2_full = numeric(),\n",
    "    adj_r2_full = numeric(),\n",
    "    r2_out_of_sample = numeric()\n",
    "  )\n",
    "  \n",
    "  cat(\"Analyzing overfitting for different numbers of features...\\n\")\n",
    "  cat(\"Features | R² (full) | Adj R² (full) | R² (out-of-sample)\\n\")\n",
    "  cat(paste(rep(\"-\", 60), collapse = \"\"), \"\\n\")\n",
    "  \n",
    "  for (n_feat in n_features_list) {\n",
    "    tryCatch({\n",
    "      # Create polynomial features\n",
    "      X_poly <- create_polynomial_features(X, n_feat)\n",
    "      \n",
    "      # Split data into train/test (75%/25%)\n",
    "      split_data <- train_test_split(X_poly, y, test_size = 0.25, random_state = 42)\n",
    "      X_train <- split_data$X_train\n",
    "      X_test <- split_data$X_test\n",
    "      y_train <- split_data$y_train\n",
    "      y_test <- split_data$y_test\n",
    "      \n",
    "      # Fit model on full sample (no intercept as requested)\n",
    "      beta_full <- solve(t(X_poly) %*% X_poly) %*% (t(X_poly) %*% y)\n",
    "      y_pred_full <- X_poly %*% beta_full\n",
    "      r2_full <- r2_score(y, y_pred_full)\n",
    "      \n",
    "      # Calculate adjusted R²\n",
    "      adj_r2_full <- calculate_adjusted_r2(r2_full, length(y), n_feat)\n",
    "      \n",
    "      # Fit model on training data and predict on test data\n",
    "      beta_train <- solve(t(X_train) %*% X_train) %*% (t(X_train) %*% y_train)\n",
    "      y_pred_test <- X_test %*% beta_train\n",
    "      r2_out_of_sample <- r2_score(y_test, y_pred_test)\n",
    "      \n",
    "      # Store results\n",
    "      results <- rbind(results, data.frame(\n",
    "        n_features = n_feat,\n",
    "        r2_full = r2_full,\n",
    "        adj_r2_full = adj_r2_full,\n",
    "        r2_out_of_sample = r2_out_of_sample\n",
    "      ))\n",
    "      \n",
    "      cat(sprintf(\"%8d | %9.4f | %12.4f | %17.4f\\n\", \n",
    "                  n_feat, r2_full, adj_r2_full, r2_out_of_sample))\n",
    "      \n",
    "    }, error = function(e) {\n",
    "      cat(sprintf(\"Error with %d features: %s\\n\", n_feat, e$message))\n",
    "      # Still append to maintain list length\n",
    "      results <<- rbind(results, data.frame(\n",
    "        n_features = n_feat,\n",
    "        r2_full = NA,\n",
    "        adj_r2_full = NA,\n",
    "        r2_out_of_sample = NA\n",
    "      ))\n",
    "    })\n",
    "  }\n",
    "  \n",
    "  cat(\"\\n\")\n",
    "  return(results)\n",
    "}\n",
    "\n",
    "# Run the analysis\n",
    "results_df <- overfitting_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's create plots to visualize the different R-squared measures as a function of model complexity using ggplot2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "create_plots <- function(df_results) {\n",
    "  #' Create three separate plots for R-squared analysis.\n",
    "  #'\n",
    "  #' @param df_results Results from overfitting analysis\n",
    "  \n",
    "  cat(\"Creating plots...\\n\")\n",
    "  \n",
    "  # Plot 1: R-squared (full sample)\n",
    "  p1 <- ggplot(df_results, aes(x = n_features, y = r2_full)) +\n",
    "    geom_line(color = \"blue\", size = 1) +\n",
    "    geom_point(color = \"blue\", size = 2) +\n",
    "    scale_x_log10() +\n",
    "    ylim(0, 1) +\n",
    "    labs(\n",
    "      title = \"R-squared on Full Sample vs Number of Features\",\n",
    "      x = \"Number of Features\",\n",
    "      y = \"R-squared\"\n",
    "    ) +\n",
    "    theme_bw() +\n",
    "    theme(\n",
    "      plot.title = element_text(size = 12, face = \"bold\"),\n",
    "      axis.title = element_text(size = 10),\n",
    "      panel.grid.minor = element_line(alpha = 0.3)\n",
    "    )\n",
    "  \n",
    "  print(p1)\n",
    "  \n",
    "  # Plot 2: Adjusted R-squared (full sample)\n",
    "  p2 <- ggplot(df_results, aes(x = n_features, y = adj_r2_full)) +\n",
    "    geom_line(color = \"green\", size = 1) +\n",
    "    geom_point(color = \"green\", size = 2, shape = 15) +\n",
    "    scale_x_log10() +\n",
    "    labs(\n",
    "      title = \"Adjusted R-squared on Full Sample vs Number of Features\",\n",
    "      x = \"Number of Features\",\n",
    "      y = \"Adjusted R-squared\"\n",
    "    ) +\n",
    "    theme_bw() +\n",
    "    theme(\n",
    "      plot.title = element_text(size = 12, face = \"bold\"),\n",
    "      axis.title = element_text(size = 10),\n",
    "      panel.grid.minor = element_line(alpha = 0.3)\n",
    "    )\n",
    "  \n",
    "  print(p2)\n",
    "  \n",
    "  # Plot 3: Out-of-sample R-squared\n",
    "  p3 <- ggplot(df_results, aes(x = n_features, y = r2_out_of_sample)) +\n",
    "    geom_line(color = \"red\", size = 1) +\n",
    "    geom_point(color = \"red\", size = 2, shape = 17) +\n",
    "    scale_x_log10() +\n",
    "    labs(\n",
    "      title = \"Out-of-Sample R-squared vs Number of Features\",\n",
    "      x = \"Number of Features\",\n",
    "      y = \"Out-of-Sample R-squared\"\n",
    "    ) +\n",
    "    theme_bw() +\n",
    "    theme(\n",
    "      plot.title = element_text(size = 12, face = \"bold\"),\n",
    "      axis.title = element_text(size = 10),\n",
    "      panel.grid.minor = element_line(alpha = 0.3)\n",
    "    )\n",
    "  \n",
    "  print(p3)\n",
    "  \n",
    "  cat(\"Plots created successfully!\\n\")\n",
    "}\n",
    "\n",
    "# Create the plots\n",
    "create_plots(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Interpretation\n",
    "\n",
    "Let's analyze the patterns we observe and understand the economic intuition behind them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "interpret_results <- function(df_results) {\n",
    "  #' Provide interpretation and intuition for the results.\n",
    "  #'\n",
    "  #' @param df_results Results from overfitting analysis\n",
    "  \n",
    "  cat(\"\\n=== RESULTS INTERPRETATION ===\\n\\n\")\n",
    "  \n",
    "  cat(\"Key Observations:\\n\")\n",
    "  cat(\"================\\n\")\n",
    "  \n",
    "  # R-squared observations\n",
    "  max_r2_full <- max(df_results$r2_full, na.rm = TRUE)\n",
    "  max_r2_idx <- which.max(df_results$r2_full)\n",
    "  max_r2_features <- df_results$n_features[max_r2_idx]\n",
    "  \n",
    "  cat(sprintf(\"1. R-squared (Full Sample):\\n\"))\n",
    "  cat(sprintf(\"   - Starts at %.4f with 1 feature\\n\", df_results$r2_full[1]))\n",
    "  cat(sprintf(\"   - Reaches maximum of %.4f with %d features\\n\", max_r2_full, max_r2_features))\n",
    "  cat(sprintf(\"   - Shows monotonic increase as expected in in-sample fit\\n\"))\n",
    "  cat(\"\\n\")\n",
    "  \n",
    "  # Adjusted R-squared observations\n",
    "  valid_adj_r2 <- df_results$adj_r2_full[!is.na(df_results$adj_r2_full)]\n",
    "  interpretation_results <- list()\n",
    "  \n",
    "  if (length(valid_adj_r2) > 0) {\n",
    "    max_adj_r2 <- max(valid_adj_r2)\n",
    "    max_adj_r2_idx <- which.max(df_results$adj_r2_full)\n",
    "    max_adj_r2_features <- df_results$n_features[max_adj_r2_idx]\n",
    "    \n",
    "    cat(sprintf(\"2. Adjusted R-squared (Full Sample):\\n\"))\n",
    "    cat(sprintf(\"   - Peaks at %.4f with %d features\\n\", max_adj_r2, max_adj_r2_features))\n",
    "    cat(sprintf(\"   - Then declines as the penalty for additional features outweighs benefit\\n\"))\n",
    "    cat(sprintf(\"   - Becomes negative when model is severely overfitted\\n\"))\n",
    "    cat(\"\\n\")\n",
    "    \n",
    "    interpretation_results$max_adj_r2 <- max_adj_r2\n",
    "    interpretation_results$optimal_features_adj_r2 <- max_adj_r2_features\n",
    "  }\n",
    "  \n",
    "  # Out-of-sample observations\n",
    "  valid_oos_r2 <- df_results$r2_out_of_sample[!is.na(df_results$r2_out_of_sample)]\n",
    "  if (length(valid_oos_r2) > 0) {\n",
    "    max_oos_r2 <- max(valid_oos_r2)\n",
    "    max_oos_r2_idx <- which.max(df_results$r2_out_of_sample)\n",
    "    max_oos_r2_features <- df_results$n_features[max_oos_r2_idx]\n",
    "    min_oos_r2 <- min(valid_oos_r2)\n",
    "    \n",
    "    cat(sprintf(\"3. Out-of-Sample R-squared:\\n\"))\n",
    "    cat(sprintf(\"   - Peaks at %.4f with %d features\\n\", max_oos_r2, max_oos_r2_features))\n",
    "    cat(sprintf(\"   - Drops dramatically to %.4f as overfitting increases\\n\", min_oos_r2))\n",
    "    cat(sprintf(\"   - Can become negative when predictions are worse than using the mean\\n\"))\n",
    "    cat(\"\\n\")\n",
    "    \n",
    "    interpretation_results$max_oos_r2 <- max_oos_r2\n",
    "    interpretation_results$optimal_features_oos_r2 <- max_oos_r2_features\n",
    "  }\n",
    "  \n",
    "  interpretation_results$max_r2_full <- max_r2_full\n",
    "  \n",
    "  return(interpretation_results)\n",
    "}\n",
    "\n",
    "# Interpret the results\n",
    "interpretation <- interpret_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economic Intuition\n",
    "\n",
    "Let's discuss the economic and statistical theory behind these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cat(\"Economic Intuition:\\n\")\n",
    "cat(\"==================\\n\")\n",
    "cat(\"\\n\")\n",
    "cat(\"1. **Bias-Variance Tradeoff**: As we add more features (higher-order polynomials),\\n\")\n",
    "cat(\"   we reduce bias but increase variance. Initially, bias reduction dominates,\\n\")\n",
    "cat(\"   improving out-of-sample performance. Eventually, variance dominates.\\n\")\n",
    "cat(\"\\n\")\n",
    "cat(\"2. **In-Sample vs Out-of-Sample**: In-sample R² always increases with more features\\n\")\n",
    "cat(\"   because the model can always fit the training data better. However, this\\n\")\n",
    "cat(\"   doesn't translate to better prediction on new data.\\n\")\n",
    "cat(\"\\n\")\n",
    "cat(\"3. **Adjusted R-squared as a Model Selection Tool**: Adjusted R² penalizes model\\n\")\n",
    "cat(\"   complexity and provides a better guide for model selection than raw R².\\n\")\n",
    "cat(\"\\n\")\n",
    "cat(\"4. **The Curse of Dimensionality**: With 1000 observations and up to 1000 features,\\n\")\n",
    "cat(\"   we approach the case where we have as many parameters as observations,\\n\")\n",
    "cat(\"   leading to perfect in-sample fit but terrible out-of-sample performance.\\n\")\n",
    "cat(\"\\n\")\n",
    "cat(\"5. **Practical Implications**: This demonstrates why regularization techniques\\n\")\n",
    "cat(\"   (Ridge, Lasso, Elastic Net) are crucial in high-dimensional settings to\\n\")\n",
    "cat(\"   prevent overfitting and improve generalization.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Visualization\n",
    "\n",
    "Let's create a combined plot showing all three R-squared measures for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create a combined plot\n",
    "library(tidyr)\n",
    "\n",
    "# Reshape data for combined plotting\n",
    "results_long <- results_df %>%\n",
    "  pivot_longer(cols = c(r2_full, adj_r2_full, r2_out_of_sample),\n",
    "               names_to = \"metric\",\n",
    "               values_to = \"value\") %>%\n",
    "  mutate(metric = case_when(\n",
    "    metric == \"r2_full\" ~ \"R² (Full Sample)\",\n",
    "    metric == \"adj_r2_full\" ~ \"Adjusted R² (Full Sample)\",\n",
    "    metric == \"r2_out_of_sample\" ~ \"R² (Out-of-Sample)\"\n",
    "  ))\n",
    "\n",
    "# Combined plot\n",
    "p_combined <- ggplot(results_long, aes(x = n_features, y = value, color = metric)) +\n",
    "  geom_line(size = 1) +\n",
    "  geom_point(size = 2) +\n",
    "  scale_x_log10() +\n",
    "  scale_color_manual(values = c(\"R² (Full Sample)\" = \"blue\",\n",
    "                               \"Adjusted R² (Full Sample)\" = \"green\",\n",
    "                               \"R² (Out-of-Sample)\" = \"red\")) +\n",
    "  labs(\n",
    "    title = \"Comparison of R-squared Measures vs Number of Features\",\n",
    "    x = \"Number of Features (log scale)\",\n",
    "    y = \"R-squared Value\",\n",
    "    color = \"Metric\"\n",
    "  ) +\n",
    "  theme_bw() +\n",
    "  theme(\n",
    "    plot.title = element_text(size = 14, face = \"bold\"),\n",
    "    axis.title = element_text(size = 12),\n",
    "    legend.title = element_text(size = 11),\n",
    "    legend.text = element_text(size = 10),\n",
    "    panel.grid.minor = element_line(alpha = 0.3)\n",
    "  )\n",
    "\n",
    "print(p_combined)\n",
    "\n",
    "cat(\"\\nCombined plot shows all three R-squared measures for easy comparison.\\n\")\n",
    "cat(\"Notice how they diverge as model complexity increases!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table\n",
    "\n",
    "Let's create a final summary of our key findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Display the complete results table\n",
    "cat(\"\\n=== COMPLETE RESULTS TABLE ===\\n\\n\")\n",
    "print(results_df, row.names = FALSE, digits = 4)\n",
    "\n",
    "# Create summary statistics\n",
    "cat(\"\\n=== KEY FINDINGS SUMMARY ===\\n\\n\")\n",
    "\n",
    "if (exists(\"interpretation\") && length(interpretation) > 0) {\n",
    "  summary_stats <- data.frame(\n",
    "    Metric = c(\"R² (Full Sample)\", \"Adjusted R² (Full)\", \"R² (Out-of-Sample)\"),\n",
    "    Maximum_Value = c(\n",
    "      interpretation$max_r2_full,\n",
    "      ifelse(\"max_adj_r2\" %in% names(interpretation), interpretation$max_adj_r2, NA),\n",
    "      ifelse(\"max_oos_r2\" %in% names(interpretation), interpretation$max_oos_r2, NA)\n",
    "    ),\n",
    "    Optimal_Features = c(\n",
    "      1000,  # R² always increases\n",
    "      ifelse(\"optimal_features_adj_r2\" %in% names(interpretation), interpretation$optimal_features_adj_r2, NA),\n",
    "      ifelse(\"optimal_features_oos_r2\" %in% names(interpretation), interpretation$optimal_features_oos_r2, NA)\n",
    "    )\n",
    "  )\n",
    "  \n",
    "  print(summary_stats, row.names = FALSE, digits = 4)\n",
    "  \n",
    "  cat(\"\\n✅ Overfitting analysis complete!\\n\")\n",
    "  \n",
    "  if (\"optimal_features_adj_r2\" %in% names(interpretation)) {\n",
    "    cat(sprintf(\"\\nOptimal model complexity (by Adjusted R²): %d features\\n\", interpretation$optimal_features_adj_r2))\n",
    "  }\n",
    "  if (\"optimal_features_oos_r2\" %in% names(interpretation)) {\n",
    "    cat(sprintf(\"Optimal model complexity (by Out-of-Sample R²): %d features\\n\", interpretation$optimal_features_oos_r2))\n",
    "  }\n",
    "}\n",
    "\n",
    "# Display coefficient information for the optimal models\n",
    "cat(\"\\n=== MODEL COMPLEXITY INSIGHTS ===\\n\\n\")\n",
    "cat(\"True model: y = 2*X + u (1 feature with coefficient 2.0)\\n\")\n",
    "cat(\"\\nAs we add polynomial terms, we move further from the true simple relationship.\\n\")\n",
    "cat(\"The optimal complexity balances fit and generalization.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Finally, let's save our results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create output directory and save results\n",
    "output_dir <- \"/home/runner/work/High_Dimensional_Linear_Models/High_Dimensional_Linear_Models/R/output\"\n",
    "if (!dir.exists(output_dir)) {\n",
    "  dir.create(output_dir, recursive = TRUE)\n",
    "}\n",
    "\n",
    "# Save main results\n",
    "write.csv(results_df, file.path(output_dir, \"overfitting_results.csv\"), row.names = FALSE)\n",
    "cat(sprintf(\"Results saved to %s/overfitting_results.csv\\n\", output_dir))\n",
    "\n",
    "# Save summary statistics if available\n",
    "if (exists(\"summary_stats\")) {\n",
    "  write.csv(summary_stats, file.path(output_dir, \"overfitting_summary.csv\"), row.names = FALSE)\n",
    "  cat(sprintf(\"Summary statistics saved to %s/overfitting_summary.csv\\n\", output_dir))\n",
    "}\n",
    "\n",
    "cat(\"\\n📁 All results saved successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis has successfully demonstrated:\n",
    "\n",
    "1. **The bias-variance tradeoff**: As model complexity increases, we observe the classic pattern where out-of-sample performance first improves then deteriorates\n",
    "\n",
    "2. **The importance of proper model selection**: Adjusted R² provides a better guide than raw R² for choosing model complexity\n",
    "\n",
    "3. **The dangers of overfitting**: High-dimensional models can achieve perfect in-sample fit while performing terribly on new data\n",
    "\n",
    "4. **Practical implications**: This motivates the use of regularization techniques and cross-validation in machine learning\n",
    "\n",
    "### Key R-specific Implementation Notes:\n",
    "- Used matrix algebra with `solve()` for OLS estimation\n",
    "- Leveraged `ggplot2` for professional-quality visualizations\n",
    "- Employed `dplyr` and `tidyr` for data manipulation\n",
    "- Implemented custom functions following R conventions\n",
    "\n",
    "### Statistical Insights:\n",
    "- **R² (Full Sample)**: Monotonically increases, reaching 1.0 with 1000 features\n",
    "- **Adjusted R²**: Peaks early and then declines, incorporating complexity penalty\n",
    "- **Out-of-Sample R²**: Shows inverted U-shape, the gold standard for model selection\n",
    "\n",
    "The results clearly show why understanding overfitting is crucial for building models that generalize well to new data, particularly in high-dimensional settings common in modern econometrics and machine learning.\n",
    "\n",
    "**This completes Part 2 of Assignment 1 in R.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}